CONFIG
â”œâ”€â”€ train
â”‚   â””â”€â”€ seed: 2222                                                              
â”‚       name: null                                                              
â”‚       interval: step                                                          
â”‚       monitor: val/accuracy                                                   
â”‚       mode: max                                                               
â”‚       ema: 0.0                                                                
â”‚       test: false                                                             
â”‚       debug: false                                                            
â”‚       ignore_warnings: false                                                  
â”‚       state:                                                                  
â”‚         mode: null                                                            
â”‚         n_context: 0                                                          
â”‚         n_context_eval: 0                                                     
â”‚       ckpt: null                                                              
â”‚       disable_dataset: false                                                  
â”‚       validate_at_start: false                                                
â”‚       pretrained_model_path: null                                             
â”‚       pretrained_model_strict_load: true                                      
â”‚       pretrained_model_state_hook:                                            
â”‚         _name_: null                                                          
â”‚       post_init_hook:                                                         
â”‚         _name_: null                                                          
â”‚       layer_decay:                                                            
â”‚         _name_: null                                                          
â”‚         decay: 0.7                                                            
â”‚                                                                               
â”œâ”€â”€ tolerance
â”‚   â””â”€â”€ logdir: ./resume                                                        
â”‚       id: null                                                                
â”‚                                                                               
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: TNLM                                                           
â”‚       group: ''                                                               
â”‚       job_type: training                                                      
â”‚       mode: online                                                            
â”‚       save_dir: .                                                             
â”‚       id: null                                                                
â”‚       name: mirnn-lin-cifar1-o                                                
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ accelerator: gpu                                                        
â”‚       strategy: null                                                          
â”‚       devices: 1                                                              
â”‚       accumulate_grad_batches: 1                                              
â”‚       max_epochs: 100                                                         
â”‚       gradient_clip_val: null                                                 
â”‚       log_every_n_steps: 1                                                    
â”‚       limit_train_batches: 1.0                                                
â”‚       limit_val_batches: 1.0                                                  
â”‚       enable_model_summary: false                                             
â”‚       track_grad_norm: 2                                                      
â”‚                                                                               
â”œâ”€â”€ loader
â”‚   â””â”€â”€ batch_size: 50                                                          
â”‚       num_workers: 4                                                          
â”‚       pin_memory: true                                                        
â”‚       drop_last: true                                                         
â”‚                                                                               
â”œâ”€â”€ dataset
â”‚   â””â”€â”€ _name_: cifar                                                           
â”‚       permute: null                                                           
â”‚       grayscale: false                                                        
â”‚       tokenize: false                                                         
â”‚       augment: false                                                          
â”‚       cutout: false                                                           
â”‚       random_erasing: false                                                   
â”‚       val_split: 0.1                                                          
â”‚       seed: 42                                                                
â”‚                                                                               
â”œâ”€â”€ task
â”‚   â””â”€â”€ _name_: base                                                            
â”‚       loss: cross_entropy                                                     
â”‚       metrics:                                                                
â”‚       - accuracy                                                              
â”‚       torchmetrics: null                                                      
â”‚                                                                               
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ _name_: adamw                                                           
â”‚       lr: 0.0003                                                              
â”‚       weight_decay: 0.05                                                      
â”‚       betas:                                                                  
â”‚       - 0.9                                                                   
â”‚       - 0.999                                                                 
â”‚                                                                               
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ _name_: cosine_warmup                                                   
â”‚       num_warmup_steps: 18000                                                 
â”‚       num_training_steps: 180000                                              
â”‚                                                                               
â”œâ”€â”€ encoder
â”‚   â””â”€â”€ linear                                                                  
â”œâ”€â”€ decoder
â”‚   â””â”€â”€ _name_: sequence                                                        
â”‚       mode: pool                                                              
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ layer:                                                                  
â”‚         cell:                                                                 
â”‚           _name_: mirnn                                                       
â”‚           hidden_activation: identity                                         
â”‚           orthogonal: false                                                   
â”‚           d_input: 384                                                        
â”‚           lr: 7.5e-05                                                         
â”‚         _name_: mirnn                                                         
â”‚         return_output: true                                                   
â”‚       _name_: model                                                           
â”‚       prenorm: false                                                          
â”‚       transposed: false                                                       
â”‚       n_layers: 1                                                             
â”‚       d_model: 384                                                            
â”‚       bidirectional: false                                                    
â”‚       residual: R                                                             
â”‚       pool: null                                                              
â”‚       norm: batch                                                             
â”‚       dropout: 0.2                                                            
â”‚       tie_dropout: false                                                      
â”‚       track_norms: true                                                       
â”‚       encoder: null                                                           
â”‚       decoder: null                                                           
â”‚                                                                               
â””â”€â”€ callbacks
    â””â”€â”€ learning_rate_monitor:                                                  
          logging_interval: step                                                
        timer:                                                                  
          step: true                                                            
          inter_step: false                                                     
          epoch: true                                                           
          val: true                                                             
        params:                                                                 
          total: true                                                           
          trainable: true                                                       
          fixed: true                                                           
        model_checkpoint:                                                       
          monitor: val/accuracy                                                 
          mode: max                                                             
          save_top_k: 1                                                         
          save_last: true                                                       
          dirpath: checkpoints/                                                 
          filename: val/accuracy                                                
          auto_insert_metric_name: false                                        
          verbose: true                                                         
        rich_model_summary:                                                     
          max_depth: 1                                                          
        rich_progress_bar:                                                      
          refresh_rate: 1                                                       
          leave: true                                                           
                                                                                
[rank: 0] Global seed set to 2222
wandb: Currently logged in as: yuqinzhou. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in ./wandb/run-20230808_151212-29yafivf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mirnn-lin-cifar1-o
wandb: â­ï¸ View project at https://wandb.ai/yuqinzhou/TNLM
wandb: ğŸš€ View run at https://wandb.ai/yuqinzhou/TNLM/runs/29yafivf
[2023-08-08 15:12:17,861][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2023-08-08 15:12:17,862][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2023-08-08 15:12:17,865][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2023-08-08 15:12:17,866][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2023-08-08 15:12:17,867][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2023-08-08 15:12:17,867][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Files already downloaded and verified
NOTE: no dropout inside recurrent cell
SequenceLightningModule(
  (model): SequenceModel(
    (drop): Identity()
    (layers): ModuleList(
      (0): SequenceResidualBlock(
        (layer): RNN(
          (cell): MIRNNCell(
            (W_hx): Linear(in_features=384, out_features=384, bias=True)
            (activate): Identity()
            (W_hh): Linear(in_features=384, out_features=384, bias=True)
          )
        )
        (residual): Residual()
        (norm): Normalization(
          (norm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop): Dropout(p=0.2, inplace=False)
        (output_linear): Sequential(
          (0): Conv1d(384, 768, kernel_size=(1,), stride=(1,))
          (1): GLU(dim=-2)
        )
        (activation): GELU(approximate=none)
      )
    )
    (norm): Identity()
  )
  (encoder): Linear(
    (0): Linear(in_features=3, out_features=384, bias=True)
  )
  (decoder): SequenceDecoder(
    (0): SequenceDecoder(
      (output_transform): Linear(in_features=384, out_features=10, bias=True)
    )
  )
)
Files already downloaded and verified
Hyperparameter groups [{'lr': 7.5e-05, 'weight_decay': 0.0}]
[2023-08-08 15:12:27,431][__main__][INFO] - Optimizer group 0 | 8 tensors | lr 0.0003 | weight_decay 0.05
[2023-08-08 15:12:27,431][__main__][INFO] - Optimizer group 1 | 4 tensors | lr 7.5e-05 | weight_decay 0.0
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name    â”ƒ Type            â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ model   â”‚ SequenceModel   â”‚  592 K â”‚
â”‚ 1 â”‚ encoder â”‚ Linear          â”‚  1.5 K â”‚
â”‚ 2 â”‚ decoder â”‚ SequenceDecoder â”‚  3.9 K â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 597 K                                                         
Non-trainable params: 0                                                         
Total params: 597 K                                                             
Total estimated model params size (MB): 2                                       
SLURM auto-requeueing enabled. Setting signal handlers.
[2023-08-08 15:12:27,446][__main__][INFO] - Loaded 'val' dataloader:         5000 examples |    100 steps
[2023-08-08 15:12:27,446][__main__][INFO] - Loaded 'test' dataloader:       10000 examples |    200 steps
[2023-08-08 15:12:35,817][__main__][INFO] - Loaded 'train' dataloader:      45000 examples |    900 steps
Epoch 0/99 â”â”â”â”â”â”â”â”â”â”â”â”â•¸    953/1200 0:08:53 â€¢        2.58it/s loss: nan v_num: 
                                     0:01:36                   fivf             
Validation â”â”â”â”â”â”â”â”         53/100   0:00:16 â€¢        3.30it/s                  
                                     0:00:15                                    
Error executing job with overrides: ['experiment=cifar/mirnn-lin-cifar', 'wandb.name=mirnn-lin-cifar1-o']
Traceback (most recent call last):
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/qvk729/Master_thesis/train.py", line 716, in <module>
    main()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/qvk729/Master_thesis/train.py", line 712, in main
    train(config)
  File "/home/qvk729/Master_thesis/train.py", line 696, in train
    trainer.fit(model)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 267, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 250, in on_advance_end
    self._run_validation()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 308, in _run_validation
    self.val_loop.run()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py", line 152, in advance
    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 137, in advance
    output = self._evaluation_step(**kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py", line 234, in _evaluation_step
    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1494, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "/home/qvk729/Master_thesis/train.py", line 445, in validation_step
    loss = self._shared_step(
  File "/home/qvk729/Master_thesis/train.py", line 323, in _shared_step
    x, y, w = self.forward(batch)
  File "/home/qvk729/Master_thesis/train.py", line 304, in forward
    x, state = self.model(x, **w, state=self._state)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/qvk729/Master_thesis/src/models/sequence/backbones/model.py", line 127, in forward
    outputs, state = layer(outputs, *args, state=prev_state, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/qvk729/Master_thesis/src/models/sequence/backbones/block.py", line 115, in forward
    y_for, new_state = self.layer(y, state=state, **kwargs) #(B, L, H)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/qvk729/Master_thesis/src/models/sequence/rnns/rnn.py", line 47, in forward
    output, new_state = self.step(input, state)
  File "/home/qvk729/Master_thesis/src/models/sequence/rnns/rnn.py", line 56, in step
    return self.cell.step(x, state)
  File "/home/qvk729/Master_thesis/src/models/sequence/rnns/cells/basic.py", line 73, in step
    return self.forward(x, state)
  File "/home/qvk729/Master_thesis/src/models/sequence/rnns/cells/basic.py", line 269, in forward
    wandb.log({"h_after/rnn": torch.mean(h)})
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 391, in wrapper
    return func(self, *args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 342, in wrapper_fn
    return func(self, *args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 332, in wrapper
    return func(self, *args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 1748, in log
    self._log(data=data, step=step, commit=commit)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 1529, in _log
    self._partial_history_callback(data, step, commit)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 1399, in _partial_history_callback
    self._backend.interface.publish_partial_history(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 586, in publish_partial_history
    self._publish_partial_history(partial_history)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 89, in _publish_partial_history
    self._publish(rec)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
wandb: While tearing down the service manager. The following error has occurred: [Errno 32] Broken pipe
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=9972.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
