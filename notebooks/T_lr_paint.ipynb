{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "LEARNING_RATE = args.lr\n",
    "EPOCHS = args.epochs\n",
    "STEPS_IN_EPOCH = 5\n",
    "\n",
    "# Set model and optimizer\n",
    "model = torch.nn.Linear(2, 1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay= args.weight_decay)\n",
    "\n",
    "# Define your scheduler here as described above\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs)\n",
    "scheduler = CosineWarmup(optimizer, T_max = args.epochs, eta_min= 1e-7, warmup_step= 2) \n",
    "\n",
    "# Get learning rates as each training step\n",
    "learning_rates = []\n",
    "\n",
    "for i in range(EPOCHS*STEPS_IN_EPOCH):\n",
    "    optimizer.step()\n",
    "    learning_rates.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "\n",
    "# Visualize learinig rate scheduler\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,5))\n",
    "ax.plot(range(EPOCHS*STEPS_IN_EPOCH), \n",
    "        learning_rates,\n",
    "        marker='o', \n",
    "        color='black')\n",
    "ax.set_xlim([0, EPOCHS*STEPS_IN_EPOCH])\n",
    "ax.set_ylim([0, LEARNING_RATE + 0.0001])\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(STEPS_IN_EPOCH))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
