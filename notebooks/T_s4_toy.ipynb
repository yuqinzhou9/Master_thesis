{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn.metrics\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now: cpu\n"
     ]
    }
   ],
   "source": [
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device available now:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 1234):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized transform (transforms to tensor, here you can normalize, perform Data Augmentation etc.)\n",
    "my_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download data\n",
    "mnist_train = torchvision.datasets.MNIST('data', train = True, download=True, transform=my_transform)\n",
    "mnist_test = torchvision.datasets.MNIST('data', train = False, download=True, transform=my_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(mnist_train), mnist_train)\n",
    "print(len(mnist_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. original images shape: torch.Size([64, 1, 28, 28])\n",
      "2. reshaped images shape: torch.Size([64, 28, 28]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Create a train_loader to select a batch from it\n",
    "train_loader_example = torch.utils.data.DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "# Taking a single batch of the images\n",
    "images, labels = next(iter(train_loader_example))\n",
    "print('1. original images shape:', images.shape)\n",
    "\n",
    "# Remove channel from shape\n",
    "images = images.reshape(-1, 28, 28)\n",
    "print('2. reshaped images shape:', images.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerRNN_MNIST(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_size, \n",
    "                 layer_size, \n",
    "                 output_size, \n",
    "                 skip_connections = True, \n",
    "                 pooling = \"mean\"):\n",
    "        \n",
    "        super(MultilayerRNN_MNIST, self).__init__()\n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size, \n",
    "        self.layer_size = layer_size, \n",
    "        self.output_size = output_size\n",
    "        self.pooling = pooling\n",
    "        self.skip_connections = skip_connections\n",
    "        self.activation = nn.GLU()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, layer_size, batch_first=True, nonlinearity='tanh')\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            self.activation,\n",
    "        )\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, images, prints=False):\n",
    "        if prints: print('images shape:', images.shape)\n",
    "        \n",
    "        # Instantiate hidden_state at timestamp 0\n",
    "        hidden_state = torch.zeros(self.layer_size, images.size(0), self.hidden_size)\n",
    "        hidden_state = hidden_state.requires_grad_()\n",
    "        if prints: print('Hidden State shape:', hidden_state.shape)\n",
    "        \n",
    "        # Compute RNN\n",
    "        output, last_hidden_state = self.rnn(images, hidden_state)\n",
    "        ### output: (batch, L, hidden_size) ### (last hidden layer)\n",
    "        ### last_hidden_state: (num_layers, batch, hidden_size) ###\n",
    "        if prints: print('RNN Output shape:', output.shape, '\\n' +\n",
    "                         'RNN last_hidden_state shape', last_hidden_state.shape)\n",
    "\n",
    "        ###  Skip connection + MLP   #### \n",
    "        if self.skip_connections:                    \n",
    "            output += self.MLP(output) \n",
    "        if prints: print('MLP Output shape:', output.shape)\n",
    "\n",
    "        ### Pooling + Output Linear ###\n",
    "        if self.pooling == \"none\":\n",
    "            output = self.output_linear(output[:, -1, :])\n",
    "            #use the last hidden state\n",
    "\n",
    "        elif self.pooling == \"mean\": \n",
    "            output = self.output_linear(torch.mean(output, axis= 1)) \n",
    "            # time pooling of all hidden state\n",
    "        if prints: print('FNN Output shape:', output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STATICS ====\n",
    "batch_size = 64\n",
    "input_size = 28\n",
    "hidden_size = 100  \n",
    "layer_size = 5         \n",
    "output_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerRNN_MNIST(\n",
      "  (activation): GLU(dim=-1)\n",
      "  (rnn): RNN(28, 100, num_layers=5, batch_first=True)\n",
      "  (MLP): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (1): GLU(dim=-1)\n",
      "  )\n",
      "  (output_linear): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "images shape: torch.Size([64, 28, 28])\n",
      "Hidden State shape: torch.Size([5, 64, 100])\n",
      "RNN Output shape: torch.Size([64, 28, 100]) \n",
      "RNN last_hidden_state shape torch.Size([5, 64, 100])\n",
      "MLP Output shape: torch.Size([64, 28, 100])\n",
      "FNN Output shape: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create model instance\n",
    "multilayer_rnn_example = MultilayerRNN_MNIST(input_size, hidden_size, layer_size, output_size)\n",
    "print(multilayer_rnn_example)\n",
    "\n",
    "\n",
    "# Making log predictions:\n",
    "out = multilayer_rnn_example(images, prints=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(out, actual_labels, batchSize):\n",
    "    '''Saves the Accuracy of the batch.\n",
    "    Takes in the log probabilities, actual label and the batchSize (to average the score).'''\n",
    "    predictions = out.max(dim=1)[1]\n",
    "    correct = (predictions == actual_labels).sum().item()\n",
    "    accuracy = correct/batch_size\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, train_data, test_data, batchSize=64, num_epochs=1, learning_rate=0.001):\n",
    "    \n",
    "    '''Trains the model and computes the average accuracy for train and test data.'''\n",
    "    \n",
    "    print('Get data ready...')\n",
    "    # Create dataloader for training dataset - so we can train on multiple batches\n",
    "    # Shuffle after every epoch\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batchSize, shuffle=True, drop_last=True)\n",
    "    \n",
    "    # Create criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    print('Training started...')\n",
    "    # Train the data multiple times\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Save Train and Test Loss\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        # Set model in training mode:\n",
    "        model.train()\n",
    "        \n",
    "        for k, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # Get rid of the channel\n",
    "            images = images.view(-1, 28, 28)\n",
    "            \n",
    "            # Create log probabilities\n",
    "            out = model(images)\n",
    "            # Clears the gradients from previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Computes loss: how far is the prediction from the actual?\n",
    "            loss = criterion(out, labels)\n",
    "            # Computes gradients for neurons\n",
    "            loss.backward()\n",
    "            # Updates the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Save Loss & Accuracy after each iteration\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_accuracy(out, labels, batchSize)\n",
    "            \n",
    "        \n",
    "        # Print Average Train Loss & Accuracy after each epoch\n",
    "        print('TRAIN | Epoch: {}/{} | Loss: {:.2f} | Accuracy: {:.2f}'.format(epoch+1, num_epochs, train_loss/k, train_acc/k))\n",
    "            \n",
    "            \n",
    "    print('Testing Started...')\n",
    "    # Save Test Accuracy\n",
    "    test_acc = 0\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for k, (images, labels) in enumerate(test_loader):\n",
    "        # Get rid of the channel\n",
    "        images = images.view(-1, 28, 28)\n",
    "        \n",
    "        # Create logit predictions\n",
    "        out = model(images)\n",
    "        # Add Accuracy of this batch\n",
    "        test_acc += get_accuracy(out, labels, batchSize)\n",
    "        \n",
    "    # Print Final Test Accuracy\n",
    "    print('TEST | Average Accuracy per {} Loaders: {:.5f}'.format(k, test_acc/k) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data ready...\n",
      "Training started...\n",
      "TRAIN | Epoch: 1/3 | Loss: 0.77 | Accuracy: 0.75\n",
      "TRAIN | Epoch: 2/3 | Loss: 0.20 | Accuracy: 0.94\n",
      "TRAIN | Epoch: 3/3 | Loss: 0.14 | Accuracy: 0.96\n",
      "Testing Started...\n",
      "TEST | Average Accuracy per 155 Loaders: 0.97409\n"
     ]
    }
   ],
   "source": [
    "# ==== STATICS ====\n",
    "batch_size = 64\n",
    "input_size = 28\n",
    "hidden_size = 100  \n",
    "layer_size = 2         \n",
    "output_size = 10\n",
    "\n",
    "# Instantiate the model\n",
    "# We'll use TANH as our activation function\n",
    "multilayer_rnn = MultilayerRNN_MNIST(input_size, hidden_size, layer_size, output_size, pooling= \"mean\")\n",
    "\n",
    "# ==== TRAIN ====\n",
    "train_network(multilayer_rnn, mnist_train, mnist_test, num_epochs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81c0a520ab9d5f38718f11fc8e39c528dfdc4be4c6d24f2eb9940d412ba4096a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
