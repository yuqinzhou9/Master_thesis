{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(f\"torch: {torch.__version__}\")\n",
    "# import torchvision\n",
    "# print(f\"torchvisio: {torchvision.__version__}\")\n",
    "# import torchaudio\n",
    "# print(f\"torchaudio: {torchaudio.__version__}\")\n",
    "# import torchtext\n",
    "# print(f\"torchtext: {torchtext.__version__}\")\n",
    "# import torchdata\n",
    "# print(f\"torchdata: {torchdata.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.2\n"
     ]
    }
   ],
   "source": [
    "print(hydra.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuqinzhou\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from src.utils.optim.schedulers import CosineWarmup\n",
    "from src.models.sequence.rnns.rnn import RNN\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now: cpu\n"
     ]
    }
   ],
   "source": [
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device available now:', device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "# Dataset\n",
    "parser.add_argument('--dataset', default='cifar10', choices=['cifar10', 'listops', 'imdb', 'aan', 'pathfinder'], type=str, help='Dataset')\n",
    "###! imdb refers to TEXT, ann refers to RETRIEVAL \n",
    "parser.add_argument('--grayscale', action='store_true', help='Use grayscale CIFAR10')\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "parser.add_argument('--num_workers', default=0, type=int, help='Number of workers to use for dataloader')\n",
    "parser.add_argument('--batch_size', default=50, type=int, help='Batch size')\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "parser.add_argument('--lr', default= 0.01, type=float, help='Learning rate') # 0.01\n",
    "parser.add_argument('--lr_factor', default= 0.25, type=float, help='Factor of Learning rate') \n",
    "parser.add_argument('--weight_decay', default=0.05, type=float, help='Weight decay')\n",
    "\n",
    "\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--epochs', default=200, type=float, help='Training epochs')\n",
    "\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--n_layers', default=2, type=int, help='Number of layers') #6\n",
    "parser.add_argument('--d_model', default=5, type=int, help='Model dimension') #512\n",
    "parser.add_argument('--d_hidden', default=5, type=int, help='Hidden (state) dimension ') #384\n",
    "parser.add_argument('--dropout', default=0.1, type=float, help='Dropout')\n",
    "parser.add_argument('--prenorm', action='store_false', help='Prenorm')\n",
    "parser.add_argument('--norm', default= 'BN', choices=['LN', 'BN'], help='Norm types')\n",
    "parser.add_argument('--cell', default= 'rnn', type=str, help='RNN\\'s cell')\n",
    "\n",
    "\n",
    "# General\n",
    "parser.add_argument('--resume', '-r', action='store_true', help='Resume from checkpoint')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=50, cell='rnn', d_hidden=5, d_model=5, dataset='cifar10', dropout=0.1, epochs=200, grayscale=False, lr=0.01, lr_factor=0.25, n_layers=2, norm='BN', num_workers=0, prenorm=True, resume=False, weight_decay=0.05)\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = RNN(d_input = 3, d_model = 5, lr = args.lr * args.lr_factor, cell = \"rnn\", return_output=True, transposed=False, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight_decay': 0.0, 'lr': 0.0025}\n",
      "{'weight_decay': 0.0, 'lr': 0.0025}\n"
     ]
    }
   ],
   "source": [
    "for i in d.parameters():\n",
    "    print(i._optim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train, val_split):\n",
    "    train_len = int(len(train) * (1.0-val_split))\n",
    "    train, val = torch.utils.data.random_split(\n",
    "        train,\n",
    "        (train_len, len(train) - train_len),\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == 'cifar10':\n",
    "    if args.grayscale:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=122.6 / 255.0, std=61.0 / 255.0),\n",
    "            transforms.Lambda(lambda x: x.view(1, 1024).t())\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            transforms.Lambda(lambda x: x.view(3, 1024).t())\n",
    "        ])\n",
    "    \n",
    "    # S4 is trained on sequences with no data augmentation!\n",
    "    transform_train = transform_test = transform\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar/', train=True, download=True, transform=transform_train)\n",
    "    trainset, _ = split_train_val(trainset, val_split=0.1)\n",
    "\n",
    "    valset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar/', train=True, download=True, transform=transform_test)\n",
    "    _, valset = split_train_val(valset, val_split=0.1)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data/cifar/', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    d_input = 3 if not args.grayscale else 1\n",
    "    d_output = 10\n",
    "\n",
    "else: raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1024, 3]) tensor([1, 9, 6, 6, 6, 9, 6, 9, 7, 3, 1, 2, 8, 0, 9, 4, 3, 8, 0, 6, 2, 3, 8, 2,\n",
      "        4, 7, 5, 3, 8, 0, 1, 3, 3, 3, 1, 4, 8, 7, 8, 0, 3, 3, 0, 1, 3, 5, 4, 0,\n",
      "        8, 2])\n"
     ]
    }
   ],
   "source": [
    "# Taking a single batch of the images\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.size(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 900)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb copy.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb%20copy.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mint\u001b[39m, torch\u001b[39m.\u001b[39m__version__\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[:\u001b[39m2\u001b[39m])) \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m11\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb%20copy.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb%20copy.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     dropout_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNbased(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output,\n",
    "        lr,\n",
    "        cell='rnn',\n",
    "        d_model=256,\n",
    "        d_hidden=128,\n",
    "        n_layers=2,\n",
    "        dropout=0.2,\n",
    "        prenorm=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm = prenorm\n",
    "\n",
    "        # Linear encoder (d_input = 1 for grayscale and 3 for RGB) (like embedding layer)\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        self.FFNs = nn.ModuleList()\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(\n",
    "                RNN(d_input = d_model, d_model = d_hidden, lr = lr, cell = cell, return_output=True, transposed=False, dropout=0)\n",
    "            )\n",
    "            # self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.norms.append(nn.BatchNorm1d(d_model)) \n",
    "            self.dropouts.append(nn.Dropout1d(dropout))\n",
    "            self.FFNs.append(nn.Sequential(nn.Linear(d_hidden, d_model*2), nn.GLU())    #shall test FFN + GELU later                  \n",
    "                                 )\n",
    "\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "        \n",
    "        for layer, norm, dropout, FFN in zip(self.layers, self.norms, self.dropouts, self.FFNs):\n",
    "            ''' Each iteration of this loop will map (B, L, d_model) -> (B, L, d_model) '''\n",
    "\n",
    "            \n",
    "            z = x #(B, L, d_model) -> (B, L, d_model)\n",
    "            if self.prenorm:\n",
    "                # Prenorm (BN)\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2) #(B, L, d_model) -> (B, L, d_model)\n",
    "\n",
    "            # Apply recurrence: we ignore the state input and output\n",
    "            z, _ = layer(z) #(B, L, d_model) -> (B, L, d_hidden)\n",
    "\n",
    "            # Dropout on the output of the Recurrence block\n",
    "            z = dropout(z) #(B, L, d_hidden) -> (B, L, d_hidden)\n",
    "            \n",
    "            # MLP +GLP\n",
    "            z = FFN(z) #(B, L, d_hidden) -> (B, L, d_model)\n",
    "\n",
    "            z = dropout(z) # (B, L, d_model) -> (B, L, d_model)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x  # (B, L, d_model) -> (B, L, d_model)\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Post-norm (BN)\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2) # (B, L, d_model) -> (B, L, d_model)\n",
    "                \n",
    "        # Pooling: average pooling over the sequence length\n",
    "        x = x.mean(dim=1) # (B, L, d_model) -> (B, d_model)\n",
    "\n",
    "        # Decode the outputs\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNbased(\n",
      "  (encoder): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): RNN(\n",
      "      (cell): RNNCell(\n",
      "        (W_hx): Linear(in_features=5, out_features=5, bias=False)\n",
      "        (activate): Tanh()\n",
      "        (W_hh): Linear(in_features=5, out_features=5, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): RNN(\n",
      "      (cell): RNNCell(\n",
      "        (W_hx): Linear(in_features=5, out_features=5, bias=False)\n",
      "        (activate): Tanh()\n",
      "        (W_hh): Linear(in_features=5, out_features=5, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropouts): ModuleList(\n",
      "    (0): Dropout1d(p=0.1, inplace=False)\n",
      "    (1): Dropout1d(p=0.1, inplace=False)\n",
      "  )\n",
      "  (FFNs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "      (1): GLU(dim=-1)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "      (1): GLU(dim=-1)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "example = RNNbased(d_input=d_input, \n",
    "                   d_output=d_output, \n",
    "                   cell='rnn',\n",
    "                   lr = args.lr * args.lr_factor,\n",
    "                   d_model=args.d_model, \n",
    "                   d_hidden=args.d_hidden, \n",
    "                   n_layers=args.n_layers, \n",
    "                   dropout=args.dropout, \n",
    "                   prenorm=args.prenorm)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizer(model, lr, weight_decay, epochs):\n",
    "    # All parameters in the model\n",
    "    all_parameters = list(model.parameters())\n",
    "\n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in all_parameters if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in all_parameters if hasattr(p, \"_optim\")]\n",
    "        # e.g., p could be {'weight_decay': 0.0, 'lr': 1e-07}\n",
    "    hps = [\n",
    "        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "    ]  # Unique dicts \n",
    "    \n",
    "    \n",
    "    for hp in hps: \n",
    "        params = [p for p in all_parameters if getattr(p, \"_optim\", None) == hp] ## select parameter matrices that have \"_optim\" and assign \"_optim = None\" to matrices that do not have\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp} ## <**hp> referes to hyperparameters e.g., {'weight_decay': 0.0}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.2)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "    scheduler = CosineWarmup(optimizer, T_max = epochs, eta_min= 1e-7, warmup_step= int(epochs * 0.1) + 1) \n",
    "\n",
    "    ''' Print optimizer info '''\n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    \n",
    "    \n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer group 0 | 12 tensors | lr 0.0004761904761904762 | weight_decay 0.05\n",
      "Optimizer group 1 | 4 tensors | lr 0.00011904761904761905 | weight_decay 0.0\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "optimizer, scheduler = setup_optimizer(\n",
    "    example, lr=args.lr, weight_decay=args.weight_decay, epochs=args.epochs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model, optimizer,  criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(trainloader))\n",
    "    print_every = 50\n",
    "\n",
    "    for batch_idx, (inputs, targets) in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        wandb.log({\"Train/Batch loss\": train_loss/(batch_idx+1)})\n",
    "\n",
    "        if (batch_idx % print_every) == 0:\n",
    "            print(f\"Batch: {batch_idx}, Avg: {train_loss/(batch_idx+1)}\")\n",
    "        \n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "            (batch_idx, len(trainloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        )\n",
    "    return train_loss/(batch_idx+1)\n",
    "\n",
    "\n",
    "\n",
    "def eval(model, criterion, dataloader):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(dataloader))\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "\n",
    "            pbar.set_description(\n",
    "                'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                (batch_idx, len(dataloader), eval_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "            )\n",
    "            \n",
    "        return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint/checkpoint_tvli55po.pth\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "run_id = wandb.util.generate_id()\n",
    "CHECKPOINT_PATH = f'./checkpoint/checkpoint_{run_id}.pth'\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wckfk5j7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da66140deb084e6c9a2adbb189e4894c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.024 MB of 0.024 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁██</td></tr><tr><td>Train/Batch loss</td><td>████▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Train/Epoch Loss</td><td>▁</td></tr><tr><td>Val/Val acc</td><td>▁</td></tr><tr><td>lr</td><td>▄▁█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Train/Batch loss</td><td>2.09261</td></tr><tr><td>Train/Epoch Loss</td><td>2.27602</td></tr><tr><td>Val/Val acc</td><td>17.14</td></tr><tr><td>lr</td><td>0.00024</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RNN_Vanilla_0</strong> at: <a href='https://wandb.ai/yuqinzhou/test/runs/wckfk5j7' target=\"_blank\">https://wandb.ai/yuqinzhou/test/runs/wckfk5j7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230511_234110-wckfk5j7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wckfk5j7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cabd0cefedf41ec9a13c19e23664858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01675181388333158, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/zhouyuqin/Desktop/Thesis/experiments/wandb/run-20230511_234802-tvli55po</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuqinzhou/test/runs/tvli55po' target=\"_blank\">RNN_Vanilla_0</a></strong> to <a href='https://wandb.ai/yuqinzhou/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuqinzhou/test' target=\"_blank\">https://wandb.ai/yuqinzhou/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuqinzhou/test/runs/tvli55po' target=\"_blank\">https://wandb.ai/yuqinzhou/test/runs/tvli55po</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model / ...\n",
      "Optimizer group 0 | 12 tensors | lr 0.0004761904761904762 | weight_decay 0.05\n",
      "Optimizer group 1 | 4 tensors | lr 0.00011904761904761905 | weight_decay 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Avg: 2.3438615798950195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50, Avg: 2.352909635095035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100, Avg: 2.331971444705925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150, Avg: 2.3215812278899137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200, Avg: 2.3131653740631406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 250, Avg: 2.3026826144214647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300, Avg: 2.293034563032892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 350, Avg: 2.286021816085207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg: 2.2779499402367267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 450, Avg: 2.270503469158435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500, Avg: 2.2634049013941113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 550, Avg: 2.2559005755478156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600, Avg: 2.2488120102049307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 650, Avg: 2.2413552118702786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg: 2.234190433749799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 750, Avg: 2.2277191736091786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 800, Avg: 2.221757888198643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 850, Avg: 2.215749137550067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (899/900) | Loss: 2.210 | Acc: 16.522% (7435/45000): : 900it [03:53,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (99/100) | Loss: 2.105 | Acc: 21.560% (1078/5000): : 100it [00:08, 11.84it/s]\n",
      "Epoch: 1 | Val acc: 21.560:   0%|          | 1/200 [04:02<13:23:44, 242.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Avg: 2.1363587379455566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50, Avg: 2.0947599013646445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100, Avg: 2.0936288810012362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150, Avg: 2.0914106447965106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200, Avg: 2.085969399456954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 250, Avg: 2.080829320200886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300, Avg: 2.0789904586500505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 350, Avg: 2.0764146754544686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg: 2.0742721370330774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 450, Avg: 2.073858916098686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500, Avg: 2.0750527891094337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 550, Avg: 2.0754631580328553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600, Avg: 2.074043715654713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 650, Avg: 2.0747376923920005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg: 2.074551219606876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 750, Avg: 2.0728394145495725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 800, Avg: 2.072138767563895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 850, Avg: 2.0720395264418228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (899/900) | Loss: 2.070 | Acc: 21.904% (9857/45000): : 900it [03:48,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (99/100) | Loss: 2.037 | Acc: 22.720% (1136/5000): : 100it [00:08, 12.22it/s]\n",
      "Epoch: 2 | Val acc: 22.720:   1%|          | 2/200 [07:59<13:08:49, 239.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Avg: 2.2228989601135254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50, Avg: 2.0424792415955486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100, Avg: 2.0458858721327076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150, Avg: 2.041804799970412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200, Avg: 2.0388152569680664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 250, Avg: 2.0401026822656276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300, Avg: 2.038749077787431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 350, Avg: 2.036048333869021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg: 2.033569275292375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 450, Avg: 2.0313177124623976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500, Avg: 2.0279384587339297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 550, Avg: 2.030109417416872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600, Avg: 2.02917235088031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 650, Avg: 2.0281574649195515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg: 2.0242219971182003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 750, Avg: 2.0230739470645687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 800, Avg: 2.023500199026234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 850, Avg: 2.0214989675618225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (899/900) | Loss: 2.020 | Acc: 24.107% (10848/45000): : 900it [03:48,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (99/100) | Loss: 1.997 | Acc: 25.640% (1282/5000): : 100it [00:08, 12.35it/s]\n",
      "Epoch: 3 | Val acc: 25.640:   2%|▏         | 3/200 [11:55<13:01:21, 237.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Avg: 1.8451147079467773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50, Avg: 1.9943430961347093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100, Avg: 1.9927877919508679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150, Avg: 1.9942537506684561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200, Avg: 1.9972219983143593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 250, Avg: 1.9954601502513505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300, Avg: 1.9949078310367674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 350, Avg: 1.9939959375267355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg: 1.994781742369445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 450, Avg: 1.995919249274514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500, Avg: 1.9961761804873834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 550, Avg: 1.9953054918784194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600, Avg: 1.9947256813826855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 650, Avg: 1.9959495442986672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg: 1.9974948222218838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 750, Avg: 1.997178655172315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 800, Avg: 1.9985641278280004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 850, Avg: 1.9981115985000737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (899/900) | Loss: 1.997 | Acc: 25.122% (11305/45000): : 900it [03:52,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (99/100) | Loss: 1.988 | Acc: 26.000% (1300/5000): : 100it [00:08, 11.99it/s]\n",
      "Epoch: 4 | Val acc: 26.000:   2%|▏         | 4/200 [15:56<13:01:11, 239.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Avg: 2.2407357692718506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50, Avg: 1.9843457165886373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100, Avg: 1.9961516656497917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150, Avg: 1.9890808198625678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200, Avg: 1.9955175219483636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 250, Avg: 2.0006637991187106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300, Avg: 2.001642516285082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 350, Avg: 1.9994675182549022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg: 1.9954101510178717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 450, Avg: 1.994510409572965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500, Avg: 1.9955311114202716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 550, Avg: 1.9967334876259528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600, Avg: 1.9962522812968682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 650, Avg: 1.9955623854873001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg: 1.9950861383266694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 750, Avg: 1.9954237509345247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 800, Avg: 1.9940524425697088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 850, Avg: 1.9944224041020968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (899/900) | Loss: 1.993 | Acc: 25.391% (11426/45000): : 900it [03:50,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (99/100) | Loss: 1.983 | Acc: 25.440% (1272/5000): : 100it [00:07, 12.62it/s]\n",
      "Epoch: 5 | Val acc: 25.440:   2%|▎         | 5/200 [19:54<12:55:53, 238.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Avg: 1.9140570163726807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50, Avg: 1.9473319053649902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100, Avg: 1.9738628049888234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150, Avg: 1.9809631227657496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 200, Avg: 1.97920061937019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 250, Avg: 1.9829230816715742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 300, Avg: 1.9854135620237585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 350, Avg: 1.9899782716718495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg: 1.9955439175156287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 450, Avg: 1.9927079688154674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 500, Avg: 1.9943043828724387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 550, Avg: 1.9939172241951724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 600, Avg: 1.994413694803806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 650, Avg: 1.9941324396616853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (700/900) | Loss: 1.994 | Acc: 25.546% (8954/35050): : 701it [02:59,  3.90it/s]\n",
      "Epoch: 5 | Val acc: 25.440:   2%|▎         | 5/200 [22:54<14:53:31, 274.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg: 1.9937379936688977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m | Val acc: \u001b[39m\u001b[39m%1.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (epoch, val_acc))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m==> Training...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m train(model \u001b[39m=\u001b[39;49m model,optimizer \u001b[39m=\u001b[39;49m optimizer, criterion \u001b[39m=\u001b[39;49m nn\u001b[39m.\u001b[39;49mCrossEntropyLoss())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m: epoch, \u001b[39m\"\u001b[39m\u001b[39mTrain/Epoch Loss\u001b[39m\u001b[39m\"\u001b[39m: epoch_loss})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m==> Validating...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb Cell 26\u001b[0m in \u001b[0;36mRNNbased.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     z \u001b[39m=\u001b[39m norm(z\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m#(B, L, d_model) -> (B, L, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# Apply recurrence: we ignore the state input and output\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m z, _ \u001b[39m=\u001b[39m layer(z) \u001b[39m#(B, L, d_model) -> (B, L, d_hidden)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Dropout on the output of the Recurrence block\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/Thesis/experiments/T_lra_cifar_wandb.ipynb#X32sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m z \u001b[39m=\u001b[39m dropout(z) \u001b[39m#(B, L, d_hidden) -> (B, L, d_hidden)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Thesis/experiments/src/models/sequence/rnns/rnn.py:60\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, inputs, state, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39munbind(inputs, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     output, new_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\u001b[39minput\u001b[39;49m, state)\n\u001b[1;32m     61\u001b[0m     state \u001b[39m=\u001b[39m new_state\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_output:\n",
      "File \u001b[0;32m~/Desktop/Thesis/experiments/src/models/sequence/rnns/rnn.py:69\u001b[0m, in \u001b[0;36mRNN.step\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, x, state):\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcell\u001b[39m.\u001b[39;49mstep(x, state)\n",
      "File \u001b[0;32m~/Desktop/Thesis/experiments/src/models/sequence/rnns/cells/basic.py:72\u001b[0m, in \u001b[0;36mCellBase.step\u001b[0;34m(self, x, state)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, x, state):\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x, state)\n",
      "File \u001b[0;32m~/Desktop/Thesis/experiments/src/models/sequence/rnns/cells/basic.py:165\u001b[0m, in \u001b[0;36mRNNCell.forward\u001b[0;34m(self, input, h)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, h):\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Update hidden state\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     hidden_preact \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_hx(\u001b[39minput\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_hh(h)\n\u001b[1;32m    166\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivate(hidden_preact)\n\u001b[1;32m    167\u001b[0m     \u001b[39m###! the last hidden state\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_runs = 1\n",
    "for run in range(total_runs):\n",
    "    wandb.init(\n",
    "        id= run_id,\n",
    "        project=\"test\", \n",
    "        # Model + Cell + Run\n",
    "        name=f\"RNN_Vanilla_{run}\", \n",
    "        config=args,\n",
    "        resume = 'allow')\n",
    "    \n",
    "    # defining model (resume or not)\n",
    "    if not wandb.run.resumed:\n",
    "        print('==> Building model / ...')\n",
    "        model = RNNbased(d_input=d_input, \n",
    "                    d_output=d_output, \n",
    "                    lr = args.lr * args.lr_factor,\n",
    "                    cell='rnn',\n",
    "                    d_model=args.d_model, \n",
    "                    d_hidden=args.d_hidden, \n",
    "                    n_layers=args.n_layers, \n",
    "                    dropout=args.dropout, \n",
    "                    prenorm=args.prenorm)\n",
    "        \n",
    "        optimizer, scheduler = setup_optimizer(model, lr=args.lr, weight_decay=args.weight_decay, epochs=args.epochs)\n",
    "        # optimizer = optim.AdamW(model.parameters(), lr= args.lr, weight_decay= args.weight_decay)\n",
    "        # scheduler = CosineWarmup(optimizer, T_max = args.epochs, eta_min= 1e-7, warmup_step= int(args.epochs * 0.1) + 1) \n",
    "        \n",
    "    else:\n",
    "        print('==> Resuming from checkpoint...')\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH) #not use wandb.restore('checkpoint.tar') because of encoding error\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        \n",
    "    model = model.to(device)\n",
    "\n",
    "    \n",
    "\n",
    "    ## defining training, validating and testing\n",
    "    pbar = tqdm(range(start_epoch, args.epochs))\n",
    "    for epoch in pbar:\n",
    "        wandb.log({\"Epoch\": epoch, \"lr_general\": scheduler.get_last_lr()[0]}) #record general lr for the current lr\n",
    "        wandb.log({\"Epoch\": epoch, \"lr_special\": scheduler.get_last_lr()[1]}) #record special lr for the current lr\n",
    "        \n",
    "        if epoch == 0:\n",
    "            pbar.set_description('Epoch: %d' % (epoch))\n",
    "        else:\n",
    "            pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "\n",
    "\n",
    "        print('==> Training...')\n",
    "        epoch_loss = train(model = model,optimizer = optimizer, criterion = nn.CrossEntropyLoss())\n",
    "        wandb.log({\"Epoch\": epoch, \"Train/Epoch Loss\": epoch_loss})\n",
    "\n",
    "\n",
    "        print('==> Validating...')\n",
    "        val_acc = eval(model = model, criterion = nn.CrossEntropyLoss(), dataloader = valloader)\n",
    "        wandb.log({\"Epoch\": epoch, \"Val/Val acc\": val_acc})\n",
    "        \n",
    "        scheduler.step() #update lr\n",
    "        \n",
    "        #Save checkpoints\n",
    "        if val_acc > best_acc:\n",
    "            state = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(), # record lr for the next epoch\n",
    "                'acc': val_acc,\n",
    "                'epoch': epoch,\n",
    "\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "                \n",
    "            torch.save(state, CHECKPOINT_PATH)\n",
    "            # wandb.save(CHECKPOINT_PATH)\n",
    "            best_acc = val_acc\n",
    "    \n",
    "    print('==> Testing...')\n",
    "    test_acc = eval(model = model, criterion = nn.CrossEntropyLoss(), dataloader = testloader)\n",
    "    wandb.log({\"Test/Test acc\": test_acc})\n",
    "    \n",
    "    # Mark the run as finished\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
