2023-05-14 12:16:27,859 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Current SDK version is 0.15.2
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Configure stats pid to 89489
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Loading settings from /Users/zhouyuqin/.config/wandb/settings
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Loading settings from /Users/zhouyuqin/Desktop/Thesis/experiments/outputs/2023-05-14/12-16-26-374705/wandb/settings
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Loading settings from environment variables: {}
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Applying setup settings: {'_disable_service': False}
2023-05-14 12:16:27,860 WARNING MainThread:89489 [wandb_setup.py:_flush():76] Could not find program at -m train
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program_relpath': None, 'program': '-m train'}
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_init.py:_log_setup():507] Logging user logs to ./wandb/run-20230514_121627-1cd7qd9n/logs/debug.log
2023-05-14 12:16:27,860 INFO    MainThread:89489 [wandb_init.py:_log_setup():508] Logging internal logs to ./wandb/run-20230514_121627-1cd7qd9n/logs/debug-internal.log
2023-05-14 12:16:27,861 INFO    MainThread:89489 [wandb_init.py:init():547] calling init triggers
2023-05-14 12:16:27,861 INFO    MainThread:89489 [wandb_init.py:init():554] wandb.init called with sweep_config: {}
config: {'train': {'seed': 0, 'name': None, 'interval': 'epoch', 'monitor': 'val/accuracy', 'mode': 'max', 'ema': 0.0, 'test': False, 'debug': False, 'ignore_warnings': False, 'state': {'mode': None, 'n_context': 0, 'n_context_eval': 0}, 'ckpt': None, 'disable_dataset': False, 'validate_at_start': False, 'pretrained_model_path': None, 'pretrained_model_strict_load': True, 'pretrained_model_state_hook': {'_name_': None}, 'post_init_hook': {'_name_': None}, 'layer_decay': {'_name_': None, 'decay': 0.7}}, 'tolerance': {'logdir': './resume', 'id': None}, 'wandb': {'project': 'hippo', 'group': '', 'job_type': 'training', 'mode': 'online', 'save_dir': '.', 'id': None}, 'trainer': {'accelerator': 'gpu', 'strategy': None, 'devices': 1, 'accumulate_grad_batches': 1, 'max_epochs': 200, 'gradient_clip_val': None, 'log_every_n_steps': 10, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'enable_model_summary': False, 'track_grad_norm': -1}, 'loader': {'batch_size': 50, 'num_workers': 4, 'pin_memory': True, 'drop_last': True}, 'dataset': {'_name_': 'mnist', 'permute': True, 'val_split': 0.1, 'seed': 42}, 'task': {'_name_': 'base', 'loss': 'cross_entropy', 'metrics': {0: 'accuracy'}, 'torchmetrics': None}, 'optimizer': {'_name_': 'adamw', 'lr': 0.001, 'weight_decay': 0.0, 'betas': {0: 0.9, 1: 0.999}}, 'scheduler': {'_name_': 'plateau', 'mode': 'max', 'factor': 0.2, 'patience': 20, 'min_lr': 0.0}, 'encoder': 'linear', 'decoder': {'_name_': 'sequence', 'mode': 'pool'}, 'model': {'layer': {'_name_': 's4', 'd_state': 64, 'channels': 1, 'bidirectional': False, 'gate': None, 'gate_act': 'id', 'bottleneck': None, 'activation': 'gelu', 'mult_act': None, 'final_act': 'glu', 'postact': None, 'initializer': None, 'weight_norm': False, 'tie_dropout': True, 'mode': 'nplr', 'init': 'legs', 'measure': None, 'rank': 1, 'dt_min': 0.001, 'dt_max': 0.1, 'dt_transform': 'softplus', 'lr': {'dt': 0.001, 'A': 0.001, 'B': 0.001}, 'wd': 0.0, 'n_ssm': 1, 'deterministic': False, 'l_max': 784, 'verbose': True}, '_name_': 'model', 'prenorm': True, 'transposed': False, 'n_layers': 4, 'd_model': 256, 'bidirectional': False, 'residual': 'R', 'pool': {'_name_': 'pool', 'stride': 1, 'expand': None}, 'norm': 'layer', 'dropout': 0.0, 'tie_dropout': True, 'track_norms': True, 'encoder': None, 'decoder': None}, 'callbacks': {'learning_rate_monitor': {'logging_interval': 'epoch'}, 'timer': {'step': True, 'inter_step': False, 'epoch': True, 'val': True}, 'params': {'total': True, 'trainable': True, 'fixed': True}, 'model_checkpoint': {'monitor': 'val/accuracy', 'mode': 'max', 'save_top_k': 1, 'save_last': True, 'dirpath': 'checkpoints/', 'filename': 'val/accuracy', 'auto_insert_metric_name': False, 'verbose': True}, 'rich_model_summary': {'max_depth': 1}, 'rich_progress_bar': {'refresh_rate': 1, 'leave': True}}}
2023-05-14 12:16:27,861 INFO    MainThread:89489 [wandb_init.py:init():596] starting backend
2023-05-14 12:16:27,861 INFO    MainThread:89489 [wandb_init.py:init():600] setting up manager
2023-05-14 12:16:27,864 INFO    MainThread:89489 [backend.py:_multiprocessing_setup():106] multiprocessing start_methods=spawn,fork,forkserver, using: fork
2023-05-14 12:16:27,867 INFO    MainThread:89489 [wandb_init.py:init():606] backend started and connected
2023-05-14 12:16:27,872 INFO    MainThread:89489 [wandb_init.py:init():700] updated telemetry
2023-05-14 12:16:27,874 INFO    MainThread:89489 [wandb_init.py:init():737] communicating run to backend with 60.0 second timeout
2023-05-14 12:16:29,014 INFO    MainThread:89489 [wandb_run.py:_on_init():2177] communicating current version
2023-05-14 12:16:29,097 INFO    MainThread:89489 [wandb_run.py:_on_init():2186] got version response 
2023-05-14 12:16:29,097 INFO    MainThread:89489 [wandb_init.py:init():787] starting run threads in backend
2023-05-14 12:16:35,917 INFO    MainThread:89489 [wandb_run.py:_console_start():2158] atexit reg
2023-05-14 12:16:35,917 INFO    MainThread:89489 [wandb_run.py:_redirect():2013] redirect: SettingsConsole.WRAP_RAW
2023-05-14 12:16:35,918 INFO    MainThread:89489 [wandb_run.py:_redirect():2078] Wrapping output streams.
2023-05-14 12:16:35,918 INFO    MainThread:89489 [wandb_run.py:_redirect():2103] Redirects installed.
2023-05-14 12:16:35,918 INFO    MainThread:89489 [wandb_init.py:init():829] run started, returning control to user process
2023-05-14 12:16:43,630 WARNING MsgRouterThr:89489 [router.py:message_loop():77] message_loop has been closed
