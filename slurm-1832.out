CONFIG
â”œâ”€â”€ train
â”‚   â””â”€â”€ seed: 2222                                                              
â”‚       name: null                                                              
â”‚       interval: step                                                          
â”‚       monitor: val/accuracy                                                   
â”‚       mode: max                                                               
â”‚       ema: 0.0                                                                
â”‚       test: false                                                             
â”‚       debug: false                                                            
â”‚       ignore_warnings: false                                                  
â”‚       state:                                                                  
â”‚         mode: null                                                            
â”‚         n_context: 0                                                          
â”‚         n_context_eval: 0                                                     
â”‚       ckpt: null                                                              
â”‚       disable_dataset: false                                                  
â”‚       validate_at_start: false                                                
â”‚       pretrained_model_path: null                                             
â”‚       pretrained_model_strict_load: true                                      
â”‚       pretrained_model_state_hook:                                            
â”‚         _name_: null                                                          
â”‚       post_init_hook:                                                         
â”‚         _name_: null                                                          
â”‚       layer_decay:                                                            
â”‚         _name_: null                                                          
â”‚         decay: 0.7                                                            
â”‚                                                                               
â”œâ”€â”€ tolerance
â”‚   â””â”€â”€ logdir: ./resume                                                        
â”‚       id: null                                                                
â”‚                                                                               
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: TNLM                                                           
â”‚       group: ''                                                               
â”‚       job_type: training                                                      
â”‚       mode: online                                                            
â”‚       save_dir: .                                                             
â”‚       id: null                                                                
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ accelerator: gpu                                                        
â”‚       strategy: null                                                          
â”‚       devices: 1                                                              
â”‚       accumulate_grad_batches: 1                                              
â”‚       max_epochs: 200                                                         
â”‚       gradient_clip_val: null                                                 
â”‚       log_every_n_steps: 10                                                   
â”‚       limit_train_batches: 1.0                                                
â”‚       limit_val_batches: 1.0                                                  
â”‚       enable_model_summary: false                                             
â”‚       track_grad_norm: -1                                                     
â”‚                                                                               
â”œâ”€â”€ loader
â”‚   â””â”€â”€ batch_size: 50                                                          
â”‚       num_workers: 4                                                          
â”‚       pin_memory: true                                                        
â”‚       drop_last: true                                                         
â”‚                                                                               
â”œâ”€â”€ dataset
â”‚   â””â”€â”€ _name_: cifar                                                           
â”‚       permute: null                                                           
â”‚       grayscale: false                                                        
â”‚       tokenize: false                                                         
â”‚       augment: false                                                          
â”‚       cutout: false                                                           
â”‚       random_erasing: false                                                   
â”‚       val_split: 0.1                                                          
â”‚       seed: 42                                                                
â”‚                                                                               
â”œâ”€â”€ task
â”‚   â””â”€â”€ _name_: base                                                            
â”‚       loss: cross_entropy                                                     
â”‚       metrics:                                                                
â”‚       - accuracy                                                              
â”‚       torchmetrics: null                                                      
â”‚                                                                               
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ _name_: adamw                                                           
â”‚       lr: 0.01                                                                
â”‚       weight_decay: 0.05                                                      
â”‚       betas:                                                                  
â”‚       - 0.9                                                                   
â”‚       - 0.999                                                                 
â”‚                                                                               
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ _name_: cosine_warmup                                                   
â”‚       num_warmup_steps: 900                                                   
â”‚       num_training_steps: 180000                                              
â”‚                                                                               
â”œâ”€â”€ encoder
â”‚   â””â”€â”€ linear                                                                  
â”œâ”€â”€ decoder
â”‚   â””â”€â”€ _name_: sequence                                                        
â”‚       mode: pool                                                              
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ layer:                                                                  
â”‚         _name_: s4                                                            
â”‚         d_state: 64                                                           
â”‚         channels: 1                                                           
â”‚         bidirectional: true                                                   
â”‚         gate: null                                                            
â”‚         gate_act: id                                                          
â”‚         bottleneck: null                                                      
â”‚         activation: gelu                                                      
â”‚         mult_act: null                                                        
â”‚         final_act: glu                                                        
â”‚         postact: null                                                         
â”‚         initializer: null                                                     
â”‚         weight_norm: false                                                    
â”‚         tie_dropout: true                                                     
â”‚         mode: nplr                                                            
â”‚         init: legs                                                            
â”‚         measure: null                                                         
â”‚         rank: 1                                                               
â”‚         dt_min: 0.001                                                         
â”‚         dt_max: 0.1                                                           
â”‚         dt_transform: softplus                                                
â”‚         lr:                                                                   
â”‚           dt: 0.001                                                           
â”‚           A: 0.001                                                            
â”‚           B: 0.001                                                            
â”‚         wd: 0.0                                                               
â”‚         n_ssm: 2                                                              
â”‚         deterministic: false                                                  
â”‚         l_max: null                                                           
â”‚         verbose: true                                                         
â”‚       _name_: model                                                           
â”‚       prenorm: false                                                          
â”‚       transposed: false                                                       
â”‚       n_layers: 6                                                             
â”‚       d_model: 512                                                            
â”‚       bidirectional: false                                                    
â”‚       residual: R                                                             
â”‚       pool:                                                                   
â”‚         _name_: pool                                                          
â”‚         stride: 1                                                             
â”‚         expand: null                                                          
â”‚       norm: layer                                                             
â”‚       dropout: 0.1                                                            
â”‚       tie_dropout: true                                                       
â”‚       track_norms: true                                                       
â”‚       encoder: null                                                           
â”‚       decoder: null                                                           
â”‚                                                                               
â””â”€â”€ callbacks
    â””â”€â”€ learning_rate_monitor:                                                  
          logging_interval: step                                                
        timer:                                                                  
          step: true                                                            
          inter_step: false                                                     
          epoch: true                                                           
          val: true                                                             
        params:                                                                 
          total: true                                                           
          trainable: true                                                       
          fixed: true                                                           
        model_checkpoint:                                                       
          monitor: val/accuracy                                                 
          mode: max                                                             
          save_top_k: 1                                                         
          save_last: true                                                       
          dirpath: checkpoints/                                                 
          filename: val/accuracy                                                
          auto_insert_metric_name: false                                        
          verbose: true                                                         
        early_stopping:                                                         
          monitor: val/acc                                                      
          mode: max                                                             
          patience: 20                                                          
          min_delta: 0                                                          
        rich_model_summary:                                                     
          max_depth: 1                                                          
        rich_progress_bar:                                                      
          refresh_rate: 1                                                       
          leave: true                                                           
                                                                                
[rank: 0] Global seed set to 2222
wandb: Currently logged in as: yuqinzhou. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.15.3
wandb: Run data is saved locally in ./wandb/run-20230520_123416-0l9u7xzm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-sea-1
wandb: â­ï¸ View project at https://wandb.ai/yuqinzhou/TNLM
wandb: ğŸš€ View run at https://wandb.ai/yuqinzhou/TNLM/runs/0l9u7xzm
[2023-05-20 12:34:25,113][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2023-05-20 12:34:25,114][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2023-05-20 12:34:25,120][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2023-05-20 12:34:25,123][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2023-05-20 12:34:25,128][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2023-05-20 12:34:25,129][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2023-05-20 12:34:25,129][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
Files already downloaded and verified
[2023-05-20 12:34:26,714][src.models.sequence.kernels.ssm][WARNING] - CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.
[2023-05-20 12:34:26,715][src.models.sequence.kernels.ssm][WARNING] - Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.
[2023-05-20 12:34:26,751][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (512, 32, None)
[2023-05-20 12:34:26,769][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (512, 32, None)
[2023-05-20 12:34:26,786][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (512, 32, None)
[2023-05-20 12:34:26,804][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (512, 32, None)
[2023-05-20 12:34:26,820][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (512, 32, None)
[2023-05-20 12:34:26,836][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (512, 32, None)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Files already downloaded and verified
Hyperparameter groups [{'weight_decay': 0.0, 'lr': 0.001}]
[2023-05-20 12:34:28,948][__main__][INFO] - Optimizer group 0 | 40 tensors | lr 0.01 | weight_decay 0.05
[2023-05-20 12:34:28,949][__main__][INFO] - Optimizer group 1 | 30 tensors | lr 0.001 | weight_decay 0.0
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name    â”ƒ Type            â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ model   â”‚ SequenceModel   â”‚  3.6 M â”‚
â”‚ 1 â”‚ encoder â”‚ Linear          â”‚  2.0 K â”‚
â”‚ 2 â”‚ decoder â”‚ SequenceDecoder â”‚  5.1 K â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 3.6 M                                                         
Non-trainable params: 0                                                         
Total params: 3.6 M                                                             
Total estimated model params size (MB): 14                                      
SLURM auto-requeueing enabled. Setting signal handlers.
[2023-05-20 12:34:28,977][__main__][INFO] - Loaded 'val' dataloader:         5000 examples |    100 steps
[2023-05-20 12:34:28,977][__main__][INFO] - Loaded 'test' dataloader:       10000 examples |    200 steps
[2023-05-20 12:34:30,251][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 1024
[2023-05-20 12:34:30,758][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 1024
[2023-05-20 12:34:30,801][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 1024
[2023-05-20 12:34:30,841][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 1024
[2023-05-20 12:34:30,882][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 1024
[2023-05-20 12:34:30,924][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 1024
[2023-05-20 12:34:31,768][__main__][INFO] - Loaded 'train' dataloader:      45000 examples |    900 steps
Epoch 0/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1200/1200 0:11:30 â€¢       4.97it/s loss: 1.25 v_num:
                                      0:00:00                  7xzm             
                                                               val/accuracy:    
                                                               0.586 val/loss:  
                                                               1.15             
                                                               test/accuracy:   
                                                               0.582 test/loss: 
                                                               1.148            
Error executing job with overrides: ['experiment=cifar/s4-cifar']
Traceback (most recent call last):
  File "/home/qvk729/Master_thesis/train.py", line 709, in main
    train(config)
  File "/home/qvk729/Master_thesis/train.py", line 693, in train
    trainer.fit(model)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.on_advance_end()
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 295, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1394, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 184, in on_train_epoch_end
    self._run_early_stopping_check(trainer)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 195, in _run_early_stopping_check
    if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 150, in _validate_condition_metric
    raise RuntimeError(error_msg)
RuntimeError: Early stopping conditioned on metric `val/acc` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `trainer/loss`, `trainer/epoch`, `norm/0`, `norm/1`, `norm/2`, `norm/3`, `norm/4`, `norm/5`, `norm/6`, `val/accuracy`, `val/loss`, `test/accuracy`, `test/loss`, `train/accuracy`, `train/loss`

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:               epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:              norm/0 â–‡â–‡â–‡â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–†â–‡â–…â–†â–…â–†â–†â–„â–„â–„â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb:              norm/1 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–
wandb:              norm/2 â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–…â–„â–…â–ƒâ–ƒâ–â–
wandb:              norm/3 â–„â–„â–„â–„â–„â–„â–…â–„â–…â–…â–…â–†â–…â–…â–…â–†â–‡â–‡â–†â–ˆâ–†â–†â–†â–‡â–‡â–†â–†â–†â–†â–…â–†â–†â–…â–…â–„â–…â–ƒâ–ƒâ–â–
wandb:              norm/4 â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–†â–†â–‡â–†â–ˆâ–†â–†â–†â–‡â–‡â–…â–‡â–‡â–‡â–†â–‡â–‡â–…â–†â–…â–‡â–…â–ƒâ–â–‚
wandb:              norm/5 â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–„â–…â–…â–„â–‡â–…â–„â–„â–†â–…â–„â–†â–†â–†â–…â–†â–†â–…â–†â–…â–ˆâ–…â–ƒâ–â–„
wandb:              norm/6 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–
wandb:       test/accuracy â–
wandb:           test/loss â–
wandb:         timer/epoch â–
wandb:          timer/step â–â–‚â–‚â–†â–‚â–„â–‚â–„â–„â–„â–ƒâ–‚â–ƒâ–‡â–„â–†â–ƒâ–„â–†â–†â–…â–†â–†â–‡â–„â–…â–…â–…â–‚â–ƒâ–…â–„â–„â–ƒâ–ƒâ–ˆâ–„â–…â–ƒâ–ˆ
wandb:    timer/validation â–â–ˆ
wandb:       trainer/epoch â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        trainer/loss â–ˆâ–‡â–†â–†â–…â–†â–†â–ƒâ–„â–…â–†â–„â–„â–„â–…â–…â–„â–„â–ƒâ–ƒâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–‚â–â–ƒâ–‚â–„â–‚â–ƒâ–‚â–â–‚
wandb:      trainer/lr/pg1 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      trainer/lr/pg2 â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:        val/accuracy â–
wandb:            val/loss â–
wandb: 
wandb: Run summary:
wandb:               epoch 0
wandb:              norm/0 0.2591
wandb:              norm/1 0.77861
wandb:              norm/2 0.80755
wandb:              norm/3 0.87303
wandb:              norm/4 0.96449
wandb:              norm/5 1.09307
wandb:              norm/6 0.1988
wandb:       test/accuracy 0.5821
wandb:           test/loss 1.14789
wandb:         timer/epoch 690.15425
wandb:          timer/step 0.26361
wandb:    timer/validation 61.24877
wandb:       trainer/epoch 0.0
wandb: trainer/global_step 900
wandb:        trainer/loss 1.30918
wandb:      trainer/lr/pg1 0.00999
wandb:      trainer/lr/pg2 0.001
wandb:        val/accuracy 0.5858
wandb:            val/loss 1.14984
wandb: 
wandb: ğŸš€ View run stellar-sea-1 at: https://wandb.ai/yuqinzhou/TNLM/runs/0l9u7xzm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230520_123416-0l9u7xzm/logs
Exception in thread NetStatThr:
Traceback (most recent call last):
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 260, in check_network_status
    self._loop_check_status(
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 216, in _loop_check_status
    local_handle = request()
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/interface/interface.py", line 795, in deliver_network_status
    return self._deliver_network_status(status)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 601, in _deliver_network_status
    return self._deliver_record(record)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py", line 560, in _deliver_record
    handle = mailbox._deliver_record(record, interface=self)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py", line 455, in _deliver_record
    interface._publish(record)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py", line 51, in _publish
    self._sock_client.send_record_publish(record)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 221, in send_record_publish
    self.send_server_request(server_req)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 155, in send_server_request
    self._send_message(msg)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 152, in _send_message
    self._sendall_with_error_handle(header + data)
  File "/home/qvk729/miniconda3/envs/TNLM/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py", line 130, in _sendall_with_error_handle
    sent = self._sock.send(data)
BrokenPipeError: [Errno 32] Broken pipe
