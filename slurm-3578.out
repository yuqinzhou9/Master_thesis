CONFIG
├── train
│   └── seed: 1111                                                              
│       name: null                                                              
│       interval: step                                                          
│       monitor: val/loss                                                       
│       mode: min                                                               
│       ema: 0.0                                                                
│       test: false                                                             
│       debug: false                                                            
│       ignore_warnings: false                                                  
│       state:                                                                  
│         mode: null                                                            
│         n_context: 0                                                          
│         n_context_eval: 0                                                     
│       ckpt: null                                                              
│       disable_dataset: false                                                  
│       validate_at_start: false                                                
│       pretrained_model_path: null                                             
│       pretrained_model_strict_load: true                                      
│       pretrained_model_state_hook:                                            
│         _name_: null                                                          
│       post_init_hook:                                                         
│         _name_: null                                                          
│       layer_decay:                                                            
│         _name_: null                                                          
│         decay: 0.7                                                            
│                                                                               
├── tolerance
│   └── logdir: ./resume                                                        
│       id: null                                                                
│                                                                               
├── wandb
│   └── project: TNLM                                                           
│       group: ''                                                               
│       job_type: training                                                      
│       mode: online                                                            
│       save_dir: .                                                             
│       id: null                                                                
│       name: s4-wt103                                                          
│                                                                               
├── trainer
│   └── accelerator: gpu                                                        
│       devices: 1                                                              
│       accumulate_grad_batches: 1                                              
│       max_epochs: 1000                                                        
│       gradient_clip_val: null                                                 
│       log_every_n_steps: 10                                                   
│       precision: 16                                                           
│       enable_model_summary: false                                             
│       track_grad_norm: -1                                                     
│       limit_train_batches: 1.0                                                
│       limit_val_batches: 1.0                                                  
│       replace_sampler_ddp: false                                              
│                                                                               
├── loader
│   └── batch_first: true                                                       
│       batch_size: 1                                                           
│       l_max: 8192                                                             
│       pad_last: false                                                         
│       n_context: 1                                                            
│       n_epoch_double: 0                                                       
│       limit_tokens: 1.0                                                       
│       eval:                                                                   
│         l_max: null                                                           
│         batch_size: null                                                      
│                                                                               
├── dataset
│   └── _name_: wt103                                                           
│       data_dir: null                                                          
│       bpe: false                                                              
│       roll_seed: 42                                                           
│       test_split: true                                                        
│                                                                               
├── optimizer
│   └── _name_: adamw                                                           
│       lr: 0.0005                                                              
│       weight_decay: 0.1                                                       
│       betas:                                                                  
│       - 0.9                                                                   
│       - 0.999                                                                 
│                                                                               
├── scheduler
│   └── _name_: cosine_warmup                                                   
│       num_warmup_steps: 1000                                                  
│       num_training_steps: 800000                                              
│                                                                               
├── task
│   └── _name_: adaptivelm                                                      
│       init_scale: 0.5                                                         
│       bias_scale: 1.0                                                         
│       div_val: 4                                                              
│       cutoffs:                                                                
│       - 19997                                                                 
│       - 39997                                                                 
│       - 199997                                                                
│       tie_weights: true                                                       
│       tie_projs:                                                              
│       - true                                                                  
│       - true                                                                  
│       - true                                                                  
│       dropemb: 0.25                                                           
│       dropsoft: 0.25                                                          
│       loss: null                                                              
│       metrics:                                                                
│       - ppl                                                                   
│                                                                               
├── encoder
│   └── None                                                                    
├── decoder
│   └── sequence                                                                
├── model
│   └── layer:                                                                  
│       - _name_: s4                                                            
│         l_max: 8192                                                           
│         final_act: glu                                                        
│         dropout: 0.25                                                         
│         lr: 0.0005                                                            
│         n_ssm: 1                                                              
│       - _name_: s4                                                            
│         l_max: 8192                                                           
│         final_act: glu                                                        
│         dropout: 0.25                                                         
│         lr: 0.0005                                                            
│         n_ssm: 1                                                              
│       - _name_: ffn                                                           
│         expand: 4                                                             
│         activation: gelu                                                      
│         dropout: 0.25                                                         
│       _name_: model                                                           
│       prenorm: true                                                           
│       transposed: false                                                       
│       n_layers: 1                                                             
│       d_model: 1024                                                           
│       bidirectional: false                                                    
│       residual: R                                                             
│       pool:                                                                   
│         _name_: pool                                                          
│         stride: 1                                                             
│         expand: null                                                          
│       norm: layer                                                             
│       dropout: 0.25                                                           
│       tie_dropout: false                                                      
│       track_norms: true                                                       
│       encoder: null                                                           
│       decoder: null                                                           
│       dropinp: 0.0                                                            
│                                                                               
└── callbacks
    └── learning_rate_monitor:                                                  
          logging_interval: step                                                
        timer:                                                                  
          step: true                                                            
          inter_step: false                                                     
          epoch: true                                                           
          val: true                                                             
        params:                                                                 
          total: true                                                           
          trainable: true                                                       
          fixed: true                                                           
        model_checkpoint:                                                       
          monitor: val/loss                                                     
          mode: min                                                             
          save_top_k: 1                                                         
          save_last: true                                                       
          dirpath: checkpoints/                                                 
          filename: val/loss                                                    
          auto_insert_metric_name: false                                        
          verbose: true                                                         
        early_stopping:                                                         
          monitor: val/accuracy                                                 
          mode: max                                                             
          patience: 100                                                         
          min_delta: 0                                                          
        rich_model_summary:                                                     
          max_depth: 1                                                          
        rich_progress_bar:                                                      
          refresh_rate: 1                                                       
          leave: true                                                           
                                                                                
[rank: 0] Global seed set to 1111
wandb: Currently logged in as: yuqinzhou. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in ./wandb/run-20230713_115308-ax30vux8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run s4-wt103
wandb: ⭐️ View project at https://wandb.ai/yuqinzhou/TNLM
wandb: 🚀 View run at https://wandb.ai/yuqinzhou/TNLM/runs/ax30vux8
[2023-07-13 11:53:12,592][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2023-07-13 11:53:12,593][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2023-07-13 11:53:12,596][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2023-07-13 11:53:12,597][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2023-07-13 11:53:12,602][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2023-07-13 11:53:12,602][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2023-07-13 11:53:12,603][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
[2023-07-13 11:53:12,641][root][INFO] - Loading cached dataset...
Vocab size: 267735
[2023-07-13 11:53:14,589][src.models.sequence.kernels.ssm][WARNING] - CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.
[2023-07-13 11:53:14,590][src.models.sequence.kernels.ssm][WARNING] - Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.
[2023-07-13 11:53:14,632][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (1024, 32, 8192)
[2023-07-13 11:53:14,681][src.models.sequence.kernels.ssm][INFO] - Constructing S4 (H, N, L) = (1024, 32, 8192)
SequenceLightningModule(
  (model): SequenceModel(
    (drop): Identity()
    (layers): ModuleList(
      (0): SequenceResidualBlock(
        (layer): S4Block(
          (layer): FFTConv(
            (activation): GELU(approximate=none)
            (kernel): SSMKernelDPLR()
            (drop): Dropout(p=0.25, inplace=False)
            (drop_kernel): Identity()
          )
          (mult_activation): Identity()
          (drop): Dropout(p=0.25, inplace=False)
          (output_linear): Sequential(
            (0): Linear(in_features=1024, out_features=2048, bias=True)
            (1): GLU(dim=-1)
          )
        )
        (residual): Residual()
        (norm): Normalization(
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (pool): DownAvgPool()
        (drop): Dropout1d(p=0.25, inplace=False)
        (output_linear): Sequential(
          (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))
          (1): GLU(dim=-2)
        )
        (activation): GELU(approximate=none)
      )
      (1): SequenceResidualBlock(
        (layer): S4Block(
          (layer): FFTConv(
            (activation): GELU(approximate=none)
            (kernel): SSMKernelDPLR()
            (drop): Dropout(p=0.25, inplace=False)
            (drop_kernel): Identity()
          )
          (mult_activation): Identity()
          (drop): Dropout(p=0.25, inplace=False)
          (output_linear): Sequential(
            (0): Linear(in_features=1024, out_features=2048, bias=True)
            (1): GLU(dim=-1)
          )
        )
        (residual): Residual()
        (norm): Normalization(
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (pool): DownAvgPool()
        (drop): Dropout1d(p=0.25, inplace=False)
        (output_linear): Sequential(
          (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))
          (1): GLU(dim=-2)
        )
        (activation): GELU(approximate=none)
      )
      (2): SequenceResidualBlock(
        (layer): FFN(
          (ff): Sequential(
            (0): Sequential(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
            )
            (1): Dropout(p=0.25, inplace=False)
            (2): Linear(in_features=4096, out_features=1024, bias=True)
          )
        )
        (residual): Residual()
        (norm): Normalization(
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (pool): DownAvgPool()
        (drop): Dropout1d(p=0.25, inplace=False)
        (output_linear): Sequential(
          (0): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,))
          (1): GLU(dim=-2)
        )
        (activation): GELU(approximate=none)
      )
    )
    (norm): Normalization(
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
  )
  (encoder): AdaptiveEmbedding(
    (0): AdaptiveEmbedding(
      (drop): Dropout(p=0.25, inplace=False)
      (emb_layers): ModuleList(
        (0): Embedding(19997, 1024)
        (1): Embedding(20000, 256)
        (2): Embedding(160000, 64)
        (3): Embedding(67738, 16)
      )
      (emb_projs): ParameterList(
          (0): Parameter containing: [torch.FloatTensor of size 1024x1024]
          (1): Parameter containing: [torch.FloatTensor of size 1024x256]
          (2): Parameter containing: [torch.FloatTensor of size 1024x64]
          (3): Parameter containing: [torch.FloatTensor of size 1024x16]
      )
    )
  )
  (decoder): SequenceDecoder(
    (0): SequenceDecoder(
      (output_transform): Identity()
    )
  )
  (loss): ProjectedAdaptiveLogSoftmax(
    (out_layers_biases): ParameterList(
        (0): Parameter containing: [torch.FloatTensor of size 19997]
        (1): Parameter containing: [torch.FloatTensor of size 20000]
        (2): Parameter containing: [torch.FloatTensor of size 160000]
        (3): Parameter containing: [torch.FloatTensor of size 67738]
    )
    (shared_out_projs): ParameterList(
        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]
        (1): Parameter containing: [torch.FloatTensor of size 1024x256]
        (2): Parameter containing: [torch.FloatTensor of size 1024x64]
        (3): Parameter containing: [torch.FloatTensor of size 1024x16]
    )
    (out_projs): OptionalParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1024x1024])
    (drop): Dropout(p=0.25, inplace=False)
  )
  (loss_val): ProjectedAdaptiveLogSoftmax(
    (out_layers_biases): ParameterList(
        (0): Parameter containing: [torch.FloatTensor of size 19997]
        (1): Parameter containing: [torch.FloatTensor of size 20000]
        (2): Parameter containing: [torch.FloatTensor of size 160000]
        (3): Parameter containing: [torch.FloatTensor of size 67738]
    )
    (shared_out_projs): ParameterList(
        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]
        (1): Parameter containing: [torch.FloatTensor of size 1024x256]
        (2): Parameter containing: [torch.FloatTensor of size 1024x64]
        (3): Parameter containing: [torch.FloatTensor of size 1024x16]
    )
    (out_projs): OptionalParameterList(  (0): Parameter containing: [torch.FloatTensor of size 1024x1024])
    (drop): Dropout(p=0.25, inplace=False)
  )
)
[2023-07-13 11:53:15,471][root][INFO] - Loading cached dataset...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Vocab size: 267735
Hyperparameter groups [{'weight_decay': 0.0, 'lr': 0.0005}]
[2023-07-13 11:53:19,166][__main__][INFO] - Optimizer group 0 | 41 tensors | lr 0.0005 | weight_decay 0.1
[2023-07-13 11:53:19,167][__main__][INFO] - Optimizer group 1 | 10 tensors | lr 0.0005 | weight_decay 0.0
┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name    ┃ Type                        ┃ Params ┃
┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ model   │ SequenceModel               │ 19.0 M │
│ 1 │ encoder │ AdaptiveEmbedding           │ 38.3 M │
│ 2 │ decoder │ SequenceDecoder             │      0 │
│ 3 │ loss    │ ProjectedAdaptiveLogSoftmax │  2.7 M │
└───┴─────────┴─────────────────────────────┴────────┘
Trainable params: 58.7 M                                                        
Non-trainable params: 0                                                         
Total params: 58.7 M                                                            
Total estimated model params size (MB): 117                                     
SLURM auto-requeueing enabled. Setting signal handlers.
eval loader: {'l_max': 8192, 'batch_size': 1, 'batch_first': True, 'pad_last': False, 'n_context': 1, 'n_epoch_double': 0, 'limit_tokens': 1.0}
eval loader: {'l_max': 8192, 'batch_size': 1, 'batch_first': True, 'pad_last': False, 'n_context': 1, 'n_epoch_double': 0, 'limit_tokens': 1.0}
[2023-07-13 11:53:20,056][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 8192
[2023-07-13 11:53:21,469][src.models.sequence.kernels.ssm][INFO] - S4: Initializing kernel to length 8192
Epoch 0/999                  0/12658 0:00:01 •        0.00it/s loss: nan v_num: 
                                     -:--:--                   vux8             
Error executing job with overrides: ['experiment=lm/s4-wt103', 'wandb.name=s4-wt103']
Traceback (most recent call last):
  File "/home/qvk729/Master_thesis/train.py", line 712, in main
    train(config)
  File "/home/qvk729/Master_thesis/train.py", line 696, in train
    trainer.fit(model)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 608, in fit
    call._call_and_handle_interrupt(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 38, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _fit_impl
    self._run(model, ckpt_path=self.ckpt_path)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1112, in _run
    results = self._run_stage()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1191, in _run_stage
    self._run_train()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1214, in _run_train
    self.fit_loop.run()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 267, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 213, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py", line 199, in run
    self.advance(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 202, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 249, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 370, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1356, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1742, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 169, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 234, in optimizer_step
    return self.precision_plugin.optimizer_step(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/native_amp.py", line 75, in optimizer_step
    closure_result = closure()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 149, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 144, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 305, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1494, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 207, in backward
    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, optimizer_idx, *args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 67, in backward
    model.backward(tensor, optimizer, optimizer_idx, *args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1486, in backward
    loss.backward(*args, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 23.64 GiB total capacity; 13.31 GiB already allocated; 3.85 GiB free; 18.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb: timer/validation ▁
wandb: 
wandb: Run summary:
wandb: timer/validation 2.80304
wandb: 
wandb: 🚀 View run s4-wt103 at: https://wandb.ai/yuqinzhou/TNLM/runs/ax30vux8
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230713_115308-ax30vux8/logs
