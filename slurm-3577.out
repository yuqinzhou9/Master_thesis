CONFIG
â”œâ”€â”€ train
â”‚   â””â”€â”€ seed: 1111                                                              
â”‚       name: null                                                              
â”‚       interval: step                                                          
â”‚       monitor: val/loss                                                       
â”‚       mode: min                                                               
â”‚       ema: 0.0                                                                
â”‚       test: false                                                             
â”‚       debug: false                                                            
â”‚       ignore_warnings: false                                                  
â”‚       state:                                                                  
â”‚         mode: null                                                            
â”‚         n_context: 0                                                          
â”‚         n_context_eval: 0                                                     
â”‚       ckpt: null                                                              
â”‚       disable_dataset: false                                                  
â”‚       validate_at_start: false                                                
â”‚       pretrained_model_path: null                                             
â”‚       pretrained_model_strict_load: true                                      
â”‚       pretrained_model_state_hook:                                            
â”‚         _name_: null                                                          
â”‚       post_init_hook:                                                         
â”‚         _name_: null                                                          
â”‚       layer_decay:                                                            
â”‚         _name_: null                                                          
â”‚         decay: 0.7                                                            
â”‚                                                                               
â”œâ”€â”€ tolerance
â”‚   â””â”€â”€ logdir: ./resume                                                        
â”‚       id: null                                                                
â”‚                                                                               
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: TNLM                                                           
â”‚       group: ''                                                               
â”‚       job_type: training                                                      
â”‚       mode: online                                                            
â”‚       save_dir: .                                                             
â”‚       id: null                                                                
â”‚       name: s4-wt103                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ accelerator: gpu                                                        
â”‚       devices: 8                                                              
â”‚       accumulate_grad_batches: 1                                              
â”‚       max_epochs: 1000                                                        
â”‚       gradient_clip_val: null                                                 
â”‚       log_every_n_steps: 10                                                   
â”‚       precision: 16                                                           
â”‚       enable_model_summary: false                                             
â”‚       track_grad_norm: -1                                                     
â”‚       limit_train_batches: 1.0                                                
â”‚       limit_val_batches: 1.0                                                  
â”‚       replace_sampler_ddp: false                                              
â”‚                                                                               
â”œâ”€â”€ loader
â”‚   â””â”€â”€ batch_first: true                                                       
â”‚       batch_size: 1                                                           
â”‚       l_max: 8192                                                             
â”‚       pad_last: false                                                         
â”‚       n_context: 1                                                            
â”‚       n_epoch_double: 0                                                       
â”‚       limit_tokens: 1.0                                                       
â”‚       eval:                                                                   
â”‚         l_max: null                                                           
â”‚         batch_size: null                                                      
â”‚                                                                               
â”œâ”€â”€ dataset
â”‚   â””â”€â”€ _name_: wt103                                                           
â”‚       data_dir: null                                                          
â”‚       bpe: false                                                              
â”‚       roll_seed: 42                                                           
â”‚       test_split: true                                                        
â”‚                                                                               
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ _name_: adamw                                                           
â”‚       lr: 0.0005                                                              
â”‚       weight_decay: 0.1                                                       
â”‚       betas:                                                                  
â”‚       - 0.9                                                                   
â”‚       - 0.999                                                                 
â”‚                                                                               
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ _name_: cosine_warmup                                                   
â”‚       num_warmup_steps: 1000                                                  
â”‚       num_training_steps: 800000                                              
â”‚                                                                               
â”œâ”€â”€ task
â”‚   â””â”€â”€ _name_: adaptivelm                                                      
â”‚       init_scale: 0.5                                                         
â”‚       bias_scale: 1.0                                                         
â”‚       div_val: 4                                                              
â”‚       cutoffs:                                                                
â”‚       - 19997                                                                 
â”‚       - 39997                                                                 
â”‚       - 199997                                                                
â”‚       tie_weights: true                                                       
â”‚       tie_projs:                                                              
â”‚       - true                                                                  
â”‚       - true                                                                  
â”‚       - true                                                                  
â”‚       dropemb: 0.25                                                           
â”‚       dropsoft: 0.25                                                          
â”‚       loss: null                                                              
â”‚       metrics:                                                                
â”‚       - ppl                                                                   
â”‚                                                                               
â”œâ”€â”€ encoder
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ decoder
â”‚   â””â”€â”€ sequence                                                                
â”œâ”€â”€ model
â”‚   â””â”€â”€ layer:                                                                  
â”‚       - _name_: s4                                                            
â”‚         l_max: 8192                                                           
â”‚         final_act: glu                                                        
â”‚         dropout: 0.25                                                         
â”‚         lr: 0.0005                                                            
â”‚         n_ssm: 1                                                              
â”‚       - _name_: s4                                                            
â”‚         l_max: 8192                                                           
â”‚         final_act: glu                                                        
â”‚         dropout: 0.25                                                         
â”‚         lr: 0.0005                                                            
â”‚         n_ssm: 1                                                              
â”‚       - _name_: ffn                                                           
â”‚         expand: 4                                                             
â”‚         activation: gelu                                                      
â”‚         dropout: 0.25                                                         
â”‚       _name_: model                                                           
â”‚       prenorm: true                                                           
â”‚       transposed: false                                                       
â”‚       n_layers: 1                                                             
â”‚       d_model: 1024                                                           
â”‚       bidirectional: false                                                    
â”‚       residual: R                                                             
â”‚       pool:                                                                   
â”‚         _name_: pool                                                          
â”‚         stride: 1                                                             
â”‚         expand: null                                                          
â”‚       norm: layer                                                             
â”‚       dropout: 0.25                                                           
â”‚       tie_dropout: false                                                      
â”‚       track_norms: true                                                       
â”‚       encoder: null                                                           
â”‚       decoder: null                                                           
â”‚       dropinp: 0.0                                                            
â”‚                                                                               
â””â”€â”€ callbacks
    â””â”€â”€ learning_rate_monitor:                                                  
          logging_interval: step                                                
        timer:                                                                  
          step: true                                                            
          inter_step: false                                                     
          epoch: true                                                           
          val: true                                                             
        params:                                                                 
          total: true                                                           
          trainable: true                                                       
          fixed: true                                                           
        model_checkpoint:                                                       
          monitor: val/loss                                                     
          mode: min                                                             
          save_top_k: 1                                                         
          save_last: true                                                       
          dirpath: checkpoints/                                                 
          filename: val/loss                                                    
          auto_insert_metric_name: false                                        
          verbose: true                                                         
        early_stopping:                                                         
          monitor: val/accuracy                                                 
          mode: max                                                             
          patience: 100                                                         
          min_delta: 0                                                          
        rich_model_summary:                                                     
          max_depth: 1                                                          
        rich_progress_bar:                                                      
          refresh_rate: 1                                                       
          leave: true                                                           
                                                                                
[rank: 0] Global seed set to 1111
wandb: Currently logged in as: yuqinzhou. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in ./wandb/run-20230713_114912-yjd3sq95
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run s4-wt103
wandb: â­ï¸ View project at https://wandb.ai/yuqinzhou/TNLM
wandb: ğŸš€ View run at https://wandb.ai/yuqinzhou/TNLM/runs/yjd3sq95
[2023-07-13 11:49:18,755][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2023-07-13 11:49:18,756][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2023-07-13 11:49:18,761][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2023-07-13 11:49:18,763][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2023-07-13 11:49:18,768][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.EarlyStopping>
[2023-07-13 11:49:18,769][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2023-07-13 11:49:18,770][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
ddp automatically configured, more than 1 gpu used!
Error executing job with overrides: ['experiment=lm/s4-wt103', 'wandb.name=s4-wt103']
Traceback (most recent call last):
  File "/home/qvk729/Master_thesis/train.py", line 712, in main
    train(config)
  File "/home/qvk729/Master_thesis/train.py", line 684, in train
    trainer = create_trainer(config)
  File "/home/qvk729/Master_thesis/train.py", line 671, in create_trainer
    trainer = pl.Trainer(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py", line 348, in insert_env_defaults
    return fn(self, **kwargs)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 420, in __init__
    self._accelerator_connector = AcceleratorConnector(
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 204, in __init__
    self._set_parallel_devices_and_init_accelerator()
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 575, in _set_parallel_devices_and_init_accelerator
    self._devices_flag = accelerator_cls.parse_devices(self._devices_flag)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py", line 82, in parse_devices
    return _parse_gpu_ids(devices, include_cuda=True)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/lightning_fabric/utilities/device_parser.py", line 104, in _parse_gpu_ids
    return _sanitize_gpu_ids(gpus, include_cuda=include_cuda, include_mps=include_mps)
  File "/home/qvk729/miniconda3/envs/TNLM_new/lib/python3.10/site-packages/lightning_fabric/utilities/device_parser.py", line 136, in _sanitize_gpu_ids
    raise MisconfigurationException(
lightning_fabric.utilities.exceptions.MisconfigurationException: You requested gpu: [0, 1, 2, 3, 4, 5, 6, 7]
 But your machine only has: [0]

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.016 MB uploaded (0.000 MB deduped)wandb: | 0.018 MB of 0.018 MB uploaded (0.000 MB deduped)wandb: ğŸš€ View run s4-wt103 at: https://wandb.ai/yuqinzhou/TNLM/runs/yjd3sq95
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230713_114912-yjd3sq95/logs
