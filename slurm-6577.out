CONFIG
â”œâ”€â”€ train
â”‚   â””â”€â”€ seed: 1111                                                              
â”‚       name: null                                                              
â”‚       interval: step                                                          
â”‚       monitor: val/loss                                                       
â”‚       mode: min                                                               
â”‚       ema: 0.0                                                                
â”‚       test: false                                                             
â”‚       debug: false                                                            
â”‚       ignore_warnings: false                                                  
â”‚       state:                                                                  
â”‚         mode: null                                                            
â”‚         n_context: 0                                                          
â”‚         n_context_eval: 0                                                     
â”‚       ckpt: null                                                              
â”‚       disable_dataset: false                                                  
â”‚       validate_at_start: false                                                
â”‚       pretrained_model_path: null                                             
â”‚       pretrained_model_strict_load: true                                      
â”‚       pretrained_model_state_hook:                                            
â”‚         _name_: null                                                          
â”‚       post_init_hook:                                                         
â”‚         _name_: null                                                          
â”‚       layer_decay:                                                            
â”‚         _name_: null                                                          
â”‚         decay: 0.7                                                            
â”‚                                                                               
â”œâ”€â”€ tolerance
â”‚   â””â”€â”€ logdir: ./resume                                                        
â”‚       id: null                                                                
â”‚                                                                               
â”œâ”€â”€ wandb
â”‚   â””â”€â”€ project: TNLM                                                           
â”‚       group: ''                                                               
â”‚       job_type: training                                                      
â”‚       mode: online                                                            
â”‚       save_dir: .                                                             
â”‚       id: null                                                                
â”‚       name: mirnn-lin-wt2                                                     
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       accumulate_grad_batches: 1                                              
â”‚       max_epochs: 200                                                         
â”‚       gradient_clip_val: null                                                 
â”‚       log_every_n_steps: 10                                                   
â”‚       precision: 16                                                           
â”‚       enable_model_summary: false                                             
â”‚       track_grad_norm: -1                                                     
â”‚       limit_train_batches: 1.0                                                
â”‚       limit_val_batches: 1.0                                                  
â”‚       replace_sampler_ddp: false                                              
â”‚                                                                               
â”œâ”€â”€ loader
â”‚   â””â”€â”€ batch_first: true                                                       
â”‚       batch_size: 50                                                          
â”‚       l_max: 128                                                              
â”‚       pad_last: false                                                         
â”‚       n_context: 1                                                            
â”‚       n_epoch_double: 0                                                       
â”‚       limit_tokens: 1.0                                                       
â”‚       eval:                                                                   
â”‚         l_max: null                                                           
â”‚         batch_size: null                                                      
â”‚                                                                               
â”œâ”€â”€ dataset
â”‚   â””â”€â”€ _name_: wt2                                                             
â”‚       data_dir: null                                                          
â”‚       bpe: true                                                               
â”‚       roll_seed: 42                                                           
â”‚       test_split: true                                                        
â”‚                                                                               
â”œâ”€â”€ optimizer
â”‚   â””â”€â”€ _name_: adamw                                                           
â”‚       lr: 0.0003                                                              
â”‚       weight_decay: 0.1                                                       
â”‚       betas:                                                                  
â”‚       - 0.9                                                                   
â”‚       - 0.999                                                                 
â”‚                                                                               
â”œâ”€â”€ scheduler
â”‚   â””â”€â”€ _name_: cosine_warmup                                                   
â”‚       num_warmup_steps: 7552                                                  
â”‚       num_training_steps: 75520                                               
â”‚                                                                               
â”œâ”€â”€ task
â”‚   â””â”€â”€ _name_: adaptivelm                                                      
â”‚       init_scale: 0.5                                                         
â”‚       bias_scale: 1.0                                                         
â”‚       div_val: 1                                                              
â”‚       cutoffs:                                                                
â”‚       - 9997                                                                  
â”‚       - 19997                                                                 
â”‚       - 29997                                                                 
â”‚       tie_weights: true                                                       
â”‚       tie_projs:                                                              
â”‚       - true                                                                  
â”‚       - true                                                                  
â”‚       - true                                                                  
â”‚       dropemb: 0.25                                                           
â”‚       dropsoft: 0.25                                                          
â”‚       loss: null                                                              
â”‚       metrics:                                                                
â”‚       - ppl                                                                   
â”‚                                                                               
â”œâ”€â”€ encoder
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ decoder
â”‚   â””â”€â”€ sequence                                                                
â”œâ”€â”€ model
â”‚   â””â”€â”€ layer:                                                                  
â”‚         cell:                                                                 
â”‚           _name_: mirnn                                                       
â”‚           hidden_activation: identity                                         
â”‚           orthogonal: false                                                   
â”‚           d_input: 384                                                        
â”‚           lr: 7.5e-05                                                         
â”‚         _name_: mirnn                                                         
â”‚         return_output: true                                                   
â”‚       _name_: model                                                           
â”‚       prenorm: false                                                          
â”‚       transposed: false                                                       
â”‚       n_layers: 1                                                             
â”‚       d_model: 384                                                            
â”‚       bidirectional: false                                                    
â”‚       residual: R                                                             
â”‚       pool: null                                                              
â”‚       norm: batch                                                             
â”‚       dropout: 0.25                                                           
â”‚       tie_dropout: false                                                      
â”‚       track_norms: false                                                      
â”‚       encoder: null                                                           
â”‚       decoder: null                                                           
â”‚                                                                               
â””â”€â”€ callbacks
    â””â”€â”€ learning_rate_monitor:                                                  
          logging_interval: step                                                
        timer:                                                                  
          step: true                                                            
          inter_step: false                                                     
          epoch: true                                                           
          val: true                                                             
        params:                                                                 
          total: true                                                           
          trainable: true                                                       
          fixed: true                                                           
        model_checkpoint:                                                       
          monitor: val/loss                                                     
          mode: min                                                             
          save_top_k: 1                                                         
          save_last: true                                                       
          dirpath: checkpoints/                                                 
          filename: val/loss                                                    
          auto_insert_metric_name: false                                        
          verbose: true                                                         
        rich_model_summary:                                                     
          max_depth: 1                                                          
        rich_progress_bar:                                                      
          refresh_rate: 1                                                       
          leave: true                                                           
                                                                                
[rank: 0] Global seed set to 1111
wandb: Currently logged in as: yuqinzhou. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.4
wandb: Run data is saved locally in ./wandb/run-20230726_235634-lx6gxbqo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mirnn-lin-wt2
wandb: â­ï¸ View project at https://wandb.ai/yuqinzhou/TNLM
wandb: ğŸš€ View run at https://wandb.ai/yuqinzhou/TNLM/runs/lx6gxbqo
[2023-07-26 23:56:39,522][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>
[2023-07-26 23:56:39,523][__main__][INFO] - Instantiating callback <src.callbacks.timer.Timer>
[2023-07-26 23:56:39,525][__main__][INFO] - Instantiating callback <src.callbacks.params.ParamsLog>
[2023-07-26 23:56:39,527][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>
[2023-07-26 23:56:39,528][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>
[2023-07-26 23:56:39,529][__main__][INFO] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>
Using 16bit None Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
vocab = OpenAIVocab()
vocab_size: 50257
[2023-07-26 23:56:40,034][root][INFO] - Loading cached dataset...
Vocab size: 50264
BPE: True
NOTE: no dropout inside recurrent cell
SequenceLightningModule(
  (model): SequenceModel(
    (drop): Identity()
    (layers): ModuleList(
      (0): SequenceResidualBlock(
        (layer): RNN(
          (cell): MIRNNCell(
            (W_hx): Linear(in_features=384, out_features=384, bias=False)
            (activate): Identity()
            (W_hh): Linear(in_features=384, out_features=384, bias=False)
          )
        )
        (residual): Residual()
        (norm): Normalization(
          (norm): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (drop): Dropout(p=0.25, inplace=False)
        (output_linear): Sequential(
          (0): Conv1d(384, 768, kernel_size=(1,), stride=(1,))
          (1): GLU(dim=-2)
        )
        (activation): GELU(approximate=none)
      )
    )
    (norm): Identity()
  )
  (encoder): AdaptiveEmbedding(
    (0): AdaptiveEmbedding(
      (drop): Dropout(p=0.25, inplace=False)
      (emb_layers): ModuleList(
        (0): Embedding(50264, 384)
      )
      (emb_projs): ParameterList()
    )
  )
  (decoder): SequenceDecoder(
    (0): SequenceDecoder(
      (output_transform): Identity()
    )
  )
  (loss): ProjectedAdaptiveLogSoftmax(
    (out_layers_biases): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 50264])
    (shared_out_projs): ParameterList()
    (out_projs): OptionalParameterList()
    (drop): Dropout(p=0.25, inplace=False)
  )
  (loss_val): ProjectedAdaptiveLogSoftmax(
    (out_layers_biases): ParameterList(  (0): Parameter containing: [torch.FloatTensor of size 50264])
    (shared_out_projs): ParameterList()
    (out_projs): OptionalParameterList()
    (drop): Dropout(p=0.25, inplace=False)
  )
)
vocab = OpenAIVocab()
vocab_size: 50257
[2023-07-26 23:56:41,672][root][INFO] - Loading cached dataset...
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Vocab size: 50264
BPE: True
Hyperparameter groups [{'lr': 7.5e-05, 'weight_decay': 0.0}]
[2023-07-26 23:56:43,806][__main__][INFO] - Optimizer group 0 | 8 tensors | lr 0.0003 | weight_decay 0.1
[2023-07-26 23:56:43,806][__main__][INFO] - Optimizer group 1 | 2 tensors | lr 7.5e-05 | weight_decay 0.0
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name    â”ƒ Type                        â”ƒ Params â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ model   â”‚ SequenceModel               â”‚  591 K â”‚
â”‚ 1 â”‚ encoder â”‚ AdaptiveEmbedding           â”‚ 19.3 M â”‚
â”‚ 2 â”‚ decoder â”‚ SequenceDecoder             â”‚      0 â”‚
â”‚ 3 â”‚ loss    â”‚ ProjectedAdaptiveLogSoftmax â”‚ 51.4 K â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 19.9 M                                                        
Non-trainable params: 0                                                         
Total params: 19.9 M                                                            
Total estimated model params size (MB): 39                                      
SLURM auto-requeueing enabled. Setting signal handlers.
Epoch 0, global step 383: 'val/loss' reached 10.29634 (best 10.29634), saving model to '/home/qvk729/Master_thesis/outputs/2023-07-26/23-56-32-958842/checkpoints/val/loss.ckpt' as top 1
eval loader: {'l_max': 128, 'batch_size': 50, 'batch_first': True, 'pad_last': False, 'n_context': 1, 'n_epoch_double': 0, 'limit_tokens': 1.0}
eval loader: {'l_max': 128, 'batch_size': 50, 'batch_first': True, 'pad_last': False, 'n_context': 1, 'n_epoch_double': 0, 'limit_tokens': 1.0}
Epoch 0/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.70it/s loss: 10.5 v_num:
                                     0:00:00                   xbqo val/ppl:    
                                                               29664.391        
                                                               val/loss: 10.296 
                                                               test/ppl:        
                                                               28633.896        
                                                               test/loss: 10.261
                                                               train/ppl:       
                                                               35828.926        
                                                               train/loss:      
                                                               10.486           
Epoch 1, global step 766: 'val/loss' reached 10.13037 (best 10.13037), saving model to '/home/qvk729/Master_thesis/outputs/2023-07-26/23-56-32-958842/checkpoints/val/loss.ckpt' as top 1
Epoch 1/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.52it/s loss: 10.3 v_num:
                                     0:00:00                   xbqo val/ppl:    
                                                               25134.535        
                                                               val/loss: 10.13  
                                                               test/ppl:        
                                                               24167.539        
                                                               test/loss: 10.09 
                                                               train/ppl:       
                                                               32826.414        
                                                               train/loss:      
                                                               10.396           
Epoch 2, global step 1149: 'val/loss' reached 9.19098 (best 9.19098), saving model to '/home/qvk729/Master_thesis/outputs/2023-07-26/23-56-32-958842/checkpoints/val/loss.ckpt' as top 1
Epoch 2/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:43 â€¢       11.40it/s loss: 9.41 v_num:
                                     0:00:00                   xbqo val/ppl:    
                                                               9827.835         
                                                               val/loss: 9.191  
                                                               test/ppl:        
                                                               9314.213         
                                                               test/loss: 9.133 
                                                               train/ppl:       
                                                               22667.725        
                                                               train/loss: 9.997
Epoch 3, global step 1532: 'val/loss' was not in top 1
Epoch 3/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:43 â€¢       11.43it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 4, global step 1915: 'val/loss' was not in top 1
Epoch 4/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:43 â€¢       11.44it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 5, global step 2298: 'val/loss' was not in top 1
Epoch 5/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.65it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 6, global step 2681: 'val/loss' was not in top 1
Epoch 6/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.67it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 7, global step 3064: 'val/loss' was not in top 1
Epoch 7/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.55it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 8, global step 3447: 'val/loss' was not in top 1
Epoch 8/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:41 â€¢       11.70it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 9, global step 3830: 'val/loss' was not in top 1
Epoch 9/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.59it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 10, global step 4213: 'val/loss' was not in top 1
Epoch 10/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.59it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 11, global step 4596: 'val/loss' was not in top 1
Epoch 11/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.59it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 12, global step 4979: 'val/loss' was not in top 1
Epoch 12/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.59it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 13, global step 5362: 'val/loss' was not in top 1
Epoch 13/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.68it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 14, global step 5745: 'val/loss' was not in top 1
Epoch 14/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:42 â€¢       11.55it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
Epoch 15, global step 6128: 'val/loss' was not in top 1
Epoch 15/199 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 471/471 0:00:41 â€¢       11.70it/s loss: nan v_num: 
                                     0:00:00                   xbqo val/ppl: nan
                                                               val/loss: nan    
                                                               test/ppl: nan    
                                                               test/loss: nan   
                                                               train/ppl: nan   
                                                               train/loss: nan  
slurmstepd: error: *** JOB 6577 ON hendrixgpu17fl CANCELLED AT 2023-07-27T00:08:51 ***
